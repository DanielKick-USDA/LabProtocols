---
title: "Python Tips"
author: "Harlow Malloc"
date: "2023-05-24"
categories: [news, code, analysis]
---

## 9/28/22 Python Trivia: Missing isn't itself

Python quirk I just learned and think is worth sharing. *A missing valued doesn't equal itself.*

Here's the context: I'm making a list of values from a column that could not be converted to a date. Missing values can't be converted so they end up in the list (e.g. `[nan, '7/5/21 for pass 2']`. So how do we discard this empty value? We use a list comprehension to see if the value is equal to itself ( `[val for val in my_list if val == val]` ) and will get a nan free list.

## 3/23/22 Caching with `pickle`

Here's a useful pattern I've been getting a lot of mileage out of lately. If you're running an analysis that has a time consuming step you can save the result as a python readable "pickle" file.

<!-- {python eval=FALSE, include=TRUE} -->

```         
import pickle as pkl

path = "./data_intermediates/processed_data.pkl"
if os.path.exists(path):
    processed_data = pkl.load(open(path, 'rb'))
else:
    # Make `processed_data` here
    pkl.dump(processed_data, open(path, 'wb'))
```

This also lets you batch a process so that you can do more with your resources. For example here's a list comprehension that will (for each day from 0-287) rearrange the weather data to be in ["long" format](http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/ "http://www.cookbook-r.com/manipulating_data/converting_data_between_wide_and_long_format/"). This is concise but requires processing the whole list at once which takes a lot of resources.

```         
sal_long_list = [_get_weather_long(results_list = res,
                                   current_day = ith_day) for ith_day in np.linspace(start = 0, stop = 287, num = 288)]
```

If we incorporate it into the pattern above we can hold fewer items in memory at a time and then merge them (e.g. with `list.extend()` ) after the fact.

```         
or ii in range(3):
    file_path = '../data/result_intermediates/sal_df_W_long_part_day'+['0-95', 
                                                                       '96-191', 
                                                                       '192-287'][ii]+'.pkl'
    if os.path.exists(file_path):
        sal_long_list = pkl.load(open(file_path, 'rb'))

    else:
        # The original list comprehension is here, 
        # just made messier by selecting a subset of the indices.
        sal_long_list = [_get_weather_long(                                
            results_list = res,
            current_day = current_day) for current_day in [
            [int(e) for e in np.linspace(start = 0, stop = 95, num = 96)],   # Batch 1
            [int(e) for e in np.linspace(start = 96, stop = 191, num = 96)], # Batch 2
            [int(e) for e in np.linspace(start = 192, stop = 287, num = 96)] # Batch 3
        ][ii]
        ]
        pkl.dump(sal_long_list, open(file_path, 'wb'))
```

## 2/16/22 For those coming from R: Silent In Place Replacement

Silent, in place assignment updating an object This tripped me up even though it's consistent with how I've seen other objects behave. I needed an attribute to hold data extracted from a collection of files in a directory and created a class for this.

```         
class hps_search_experiment:
    def __init__(self, path="", trial_type=''):
        self.path = path
        self.trial_type = trial_type
        self.hps_best_trial = None
        
    def process_hps_files(self):
        # ...
        
        self.hps_best_trial = hps_best_trial
```

However, running like so fails.

```         
test = hps_search_experiment(
    path = './hps_search_intermediates_G/', 
    trial_type = 'rnr')
    
test = test.process_hps_files()
test.hps_best_trial

#> AttributeError: 'NoneType' object has no attribute 'hps_best_trial'
```

This had me baffeld because I was thinking with R's norms of `data <- data %>% funciton()` where in place replacement is the exception. Instead I needed to be thinking with python's base object norms (e.g. `a_list.extend(['b', 'c'])` ). This fails because I overwrote `test` with the output of the method, which returns notthing since it's overwriting attributes within `test`'s scope.

These would also work to update the attribute:

```         
self.hps_best_trial = hps_best_trial

hps_search_experiment.__setattr__(self, "hps_best_trial", hps_best_trial)

# if it's initialized as a list
self.hps_best_trial.append([hps_best_trial]) 
# if a dict is initialized for data
self.data = {'a':1}
self.data.update({'hps_best_trial':hps_best_trial})        
```

## 7/13/21 Reusing custom functions

I wanted to reuse a custom function across a few scripts without having copies of the same code in each script. The solution I found is to set up a [module](https://docs.python.org/3/tutorial/modules.html#packages "https://docs.python.org/3/tutorial/modules.html#packages") to hold these functions. This seems straightforward once you know how it's done. 

1.  Set up a directory containing your functions and a blank file called `__init__.py`. 

    ![]()![](Picture1.jpg)

2.   Add the directory containing your module directory to the system path (here `MaizeModel` instead of `MaizeModel\\library`). If you're on OSX or linux you'll probably use single forward slashes instead of double backslashes.

    ![](Picture2.jpg)

3.  Finally import and call your functions.

Caveats: 

-   It seems that the system path isn't permanently altered by `sys.path.append`, so one would need that at the start of the script or modify it some other way.
-   If your custom functions are in the in the same directory as your script, I think you can skip all of this and just import them.
-   If your functions are in a sub-directory of the same directory as your script, I think you can get away without adding the directory to the path.

## 6/16/21 Making data easier to read

```         
# [In]
print(results_dictionary)
print("\n --------------------------------------------- \n")
import pprint
pprint.PrettyPrinter(indent=4).pprint(results_dictionary)
# [Out]
{'active': True, 'additionalInfo:programDbId': '343', 'additionalInfo:programName': 'UC Davis', 'commonCropName': 'Cassava', 'contacts': None, 'culturalPractices': None, 'dataLinks': [], 
# ...
'trialDbId': '343', 'trialName': 'UC Davis'}
--------------------------------------------- 
{   'active': True,
    'additionalInfo:programDbId': '343',    'additionalInfo:programName': 'UC Davis',    'commonCropName': 'Cassava',    'contacts': None,    'culturalPractices': None,    'dataLinks': [],    # ...    'trialDbId': '343',    'trialName': 'UC Davis'}
```

Here's a tool that some may find useful when working with data that's not yet in a `[DataFrame]`. It lets one "[pretty-print](https://docs.python.org/3/library/pprint.html "https://docs.python.org/3/library/pprint.html")" an object making any text that would wrap easier to read.

6/15/21 [This](https://www.python-graph-gallery.com/ "https://www.python-graph-gallery.com/") or similar sites can be helpful for looking up the right code/library for a plot. You can also find library specific ones. ([matplotlib](https://matplotlib.org/stable/gallery/index.html "https://matplotlib.org/stable/gallery/index.html"), [plotly](https://plotly.com/python/ "https://plotly.com/python/"))

One of R's main plotting libraries, ggplot2, describes plots by layering one component on top of another (e.g. starting with x and y variables, adding points, adding error bars, adding aesthetic adjustments). If that sort of approach appeals to you there is a python version of this library called plotnine ([github](https://github.com/has2k1/plotnine "https://github.com/has2k1/plotnine"), [example use](https://datacarpentry.org/python-ecology-lesson/07-visualization-ggplot-python/index.html "https://datacarpentry.org/python-ecology-lesson/07-visualization-ggplot-python/index.html")).

## 6/9/21 Jupyter Plugins

I came across a handy set of tools for jupyter. There are of extensions for the notebooks that give you access to code snips, autocomplete by default, rendering a notebook as a slide show, and other features. To get it installed within an anaconda virtual environment you may only need to install it with this command:

`conda install -c conda-forge jupyter_contrib_nbextensions`

I not all of the extensions were showing up for me until I also ran these two lines, so it may take a bit of fiddling to get it to run.

```         
jupyter contrib nbextension install --user
jupyter nbextension enable codefolding/main
```

Here's a [link](https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29 "https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29")to a page that shows some of these extensions in action.
