---
title: "Deep Learning Discussion: The Bigram Language Model"
subtitle: "Part 2"
author: "Daniel Kick"
date: "2024-11-12"
image: "https://upload.wikimedia.org/wikipedia/commons/a/ad/Cuneiform_tablet-_private_letter_MET_ME66_245_2.jpg"
categories: 
  - beginner
  - code
  - Deep Learning
freeze: true
---

## For Next Session:

-   Explore the Bigram Model â€“ Try to improve on the Bigram model we wrote this week. How much is performance influenced by...

    -   Optimizer choice?

    -   Learning rate?

    -   Number and size of hidden layers?

    -   Batch size?

    -   Randomness of training set

    -   ...

-   Next session we will be discussing attention and transformers. For next session:

    -   Get an introduction to transormers and attention with Grant Sanderson's [But what is a GTP 0h27](https://www.3blue1brown.com/lessons/gpt) and [Visualizing Attention 0h26](https://www.3blue1brown.com/lessons/attention).

    -   Then work through [Generatively Pretrained Transformer 1h56](https://www.youtube.com/watch?v=kCc8FmEb1nY) following along in a notebook.

    -   Finally (optionally) watch [How might LLMs store facts 0h22](https://www.3blue1brown.com/lessons/mlp).

## Materials

-   Here is the notebook we finished writing today [(Demo Notebook Part 2)](./InClassExample_Bigram_pt2.ipynb.tar.gz).
