[
  {
    "objectID": "protocols/Rootbot/Using_Sorting_Script/index.html",
    "href": "protocols/Rootbot/Using_Sorting_Script/index.html",
    "title": "Using the Image Sorting Script",
    "section": "",
    "text": "Log into rootbot ssh pi\\@10.206.31.189\nMove to Pictures cd Pictures/\n\n\nLook in the folder with ls – There will either be loose jpgs or they will be packaged into archive folders with the form “YYYYMMDD-Archive”\n\nLog into lambda2 ssh labmember\\@MW22-lambda2\nOn lambda2 move to the inbox cd Documents/rootbot_dev/inst/extdata/Pictures/Inbox\n\nWe expect this inbox to be empty (but it’s okay if there are some jpgs here)\n\n\nFrom the rootbot, copy the files over to lambda2\n\nIf the jpgs are loose run (all one command) rsync -azv ./\\*.jpg labmember\\@10.206.28.81:\\~/Documents/rootbot_dev/inst/extdata/Pictures/Inbox/\n\n\n\nThen make a folder to archive the jpgs in with mkdir$-Archive\nThen copy each day’s files like so mv\\(\\color{red}{202209}`\\*.jpg ./`\\)-Archive/\n\nNote! The red above will need to be customized\nNote! If multiple experiments were run then you’ll need to specify each day that needs to be moved into each archive.\n\nmv 20220922*.jpg ./20220922-Archive/\nmv 20220923*.jpg ./20220923-Archive/\n…\nmv 20220929*.jpg ./20220929-Archive/\nmv 20220930*.jpg ./20220930-Archive/\n\n\n\nIf the jpgs are not loose copy the jpgs from an archive (specify the archive name highlighted below) rsync -azv ./$/\\*.jpg labmember\\@10.206.28.81:\\~/Documents/rootbot_dev/inst/extdata/Pictures/Inbox/\nThe terminal session on rootbot is no longer needed and can be closed.\nConfirm the files have been moved with ls on lambda2\n\n\n\nOn lambda2, move back to “rootbot_dev” with cd \\~/Documents/rootbot_dev\nOn lambda2, start up jupyter without a browser jupyter notebook --no-browser --port=8896\n\n\nOpen a new terminal and connect the jupyter notebook to your machine ssh -N -f -Y -L 8896:localhost:8896 labmember\\@MW22-lambda2\n\n\n\n\n\nOpen a browser and go to http://localhost:8896 (it will change to the url below if everything is okay).\n\nIf you get a login page, you’ll need to copy and paste the token from step 10\n\n\n\nIf you get a different error take a screenshot so we can debug it.\n\nThe screen should now look like this:\n\n\n\nGo to scripts\n\n\n\nOpen the “sortmaster2000”\n\n\n\nRun the script cell by cell until you get to …\n\n\n\n… until you get to this cell which contains instructions on how to get the files to your local computer\n\n\n\nOpen a terminal on lambda2\n\nIf you want to run this command on lambda2\nIt’s okay to open a new ssh session as in step 3\n\n\nIt’s also okay to reuse the terminal running the jupyter notebook. If you want to do this, press “control c”\n\n\n\nFollow the instructions in 17\n\n\nIf you want to run this command locally, on your local machine run rsync -azv --files-from:= labmember\\@MW22-lambda2:\\~/Documents/rootbot_dev/inst/extdata/Pictures/Experiments/\\(\\color{red}{20220915}\\)/send_files.txt ./$\nNote: The red above will need to be customized. The folder will need to be set to the current value and ../dest/ will need to be customized to the location on your computer you want."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html",
    "href": "protocols/Operations/Website_Overview/index.html",
    "title": "Website Overview",
    "section": "",
    "text": "The easiest way to get the desktop version working is to create a new website or blog and render it. Any missing packages will cause an error. After it builds successfully, you can discard the newly made project directory and switch to this one.\n\nIn RStudio, select File &gt; New Project &gt; New Directory and select either Quarto Website or Quarto Blog.\n\nFollow the remaining prompts placing the project directory in a location where it will be easy to remove.\nThe template site will look like this. Since the focus here is checking dependencies we won’t go into what each of these documents are but Quarto’s documentation is quite good if you would like to learn more.\n\nNote that there are two render buttons. The one on the top pane renders a single document. This is useful to check how a document looks without rebuilding the entire website or for updating a document that is “frozen” and will not be re-rendered when the website is built. The second one under the build tab and will setup the website when run. Render index.qmd now and address any errors (missing libraries) that come up.\nNow render the website. Your default web browser should open to localhost:#### . The page will look like this: \nNow you are ready to build the documentation site! You can close the RStudio project, delete it, and switch to the website repository.\n\n\n\n\nThis is a terse variant of these instructions.\nmkdir rstudio_container && cd rstudio_container\n\n# Add necessary subfolders -----------------------------------------------------\nmkdir -p run var-lib-rstudio-server\nprintf 'provider=sqlite\\ndirectory=/var/lib/rstudio-server\\n' &gt; database.conf\nmkdir home\n\n# Add in preferences -----------------------------------------------------------\nmkdir ide_settings\n# if you're running from within the WSL\n# cp /mnt/c/Users/&lt;USER NAME&gt;/AppData/Roaming/RStudio/rstudio-prefs.json ./ide_settings/\n# if you're running from OSX/Linux\n# cp ~/.config/rstudio/rstudio-prefs.json ./ide_settings/\n\n# Create container with publishing capabilities --------------------------------\necho \"Bootstrap: docker\\nFrom: rocker/verse:4.2\" &gt; RStudio.def\nsudo singularity build RStudio.sif RStudio.def \n\n# Initial run without preferences ----------------------------------------------\nsingularity exec \\\n  --bind run:/run \\\n  --bind var-lib-rstudio-server:/var/lib/rstudio-server \\\n  --bind database.conf:/etc/rstudio/database.conf \\\n  --bind home:/home \\\n  --bind /mnt/c/Users/drk8b9/Documents/LabProtocols:/home/rstudio/LabProtocols \\\n  --bind ide_settings:/home/rstudio/.config/rstudio/ \\\n  RStudio.sif \\\n  rserver \\\n  --www-address=127.0.0.1 \\\n  --www-port=8700 \\\n  --server-user=rstudio\n  \n# Then shutdown server."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#desktop-version",
    "href": "protocols/Operations/Website_Overview/index.html#desktop-version",
    "title": "Website Overview",
    "section": "",
    "text": "The easiest way to get the desktop version working is to create a new website or blog and render it. Any missing packages will cause an error. After it builds successfully, you can discard the newly made project directory and switch to this one.\n\nIn RStudio, select File &gt; New Project &gt; New Directory and select either Quarto Website or Quarto Blog.\n\nFollow the remaining prompts placing the project directory in a location where it will be easy to remove.\nThe template site will look like this. Since the focus here is checking dependencies we won’t go into what each of these documents are but Quarto’s documentation is quite good if you would like to learn more.\n\nNote that there are two render buttons. The one on the top pane renders a single document. This is useful to check how a document looks without rebuilding the entire website or for updating a document that is “frozen” and will not be re-rendered when the website is built. The second one under the build tab and will setup the website when run. Render index.qmd now and address any errors (missing libraries) that come up.\nNow render the website. Your default web browser should open to localhost:#### . The page will look like this: \nNow you are ready to build the documentation site! You can close the RStudio project, delete it, and switch to the website repository."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#containerized-version",
    "href": "protocols/Operations/Website_Overview/index.html#containerized-version",
    "title": "Website Overview",
    "section": "",
    "text": "This is a terse variant of these instructions.\nmkdir rstudio_container && cd rstudio_container\n\n# Add necessary subfolders -----------------------------------------------------\nmkdir -p run var-lib-rstudio-server\nprintf 'provider=sqlite\\ndirectory=/var/lib/rstudio-server\\n' &gt; database.conf\nmkdir home\n\n# Add in preferences -----------------------------------------------------------\nmkdir ide_settings\n# if you're running from within the WSL\n# cp /mnt/c/Users/&lt;USER NAME&gt;/AppData/Roaming/RStudio/rstudio-prefs.json ./ide_settings/\n# if you're running from OSX/Linux\n# cp ~/.config/rstudio/rstudio-prefs.json ./ide_settings/\n\n# Create container with publishing capabilities --------------------------------\necho \"Bootstrap: docker\\nFrom: rocker/verse:4.2\" &gt; RStudio.def\nsudo singularity build RStudio.sif RStudio.def \n\n# Initial run without preferences ----------------------------------------------\nsingularity exec \\\n  --bind run:/run \\\n  --bind var-lib-rstudio-server:/var/lib/rstudio-server \\\n  --bind database.conf:/etc/rstudio/database.conf \\\n  --bind home:/home \\\n  --bind /mnt/c/Users/drk8b9/Documents/LabProtocols:/home/rstudio/LabProtocols \\\n  --bind ide_settings:/home/rstudio/.config/rstudio/ \\\n  RStudio.sif \\\n  rserver \\\n  --www-address=127.0.0.1 \\\n  --www-port=8700 \\\n  --server-user=rstudio\n  \n# Then shutdown server."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#project-root",
    "href": "protocols/Operations/Website_Overview/index.html#project-root",
    "title": "Website Overview",
    "section": "Project Root",
    "text": "Project Root\nLet’s look at this project from a high level. In this directory there’s an Rproj (that you’ve opened), documents for the landing page (index.qmd) and about (about.qmd) pages, directories for protocols and info on people, documents that control the website style and layour (_quarto.yml, styles.css), and a folder that contains the generated html pages (_site).\nLabProtocols\n├── LabProtocols.Rproj\n├── _quarto.yml\n├── _site\n├── about.qmd\n├── index.qmd\n├── people\n├── protocols\n└── styles.css\nThe index.qmd file contains links to protocols. Whenever new pages are added they should be linked here to be visible. The about.qmd file has information on the lab and it’s members. Lab member pictures are pulled from /people/ so any new pictures should be added there along with any personal pages (e.g. if we want to link student resumes). Additional folders could be added to hold assests that may be useful. For instance, we could add a /papers/ folder and link from index.qmd to the lab’s papers. This would be helpful for on boarding."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#protocols",
    "href": "protocols/Operations/Website_Overview/index.html#protocols",
    "title": "Website Overview",
    "section": "Protocols",
    "text": "Protocols\nThe protocols folder is where most of the work happens. Inside it there are subfolders to roughly group protocols. Right now it contains Drones, Logistics, and Operations. Other useful groups might include Rootbot, Wet Lab, and Field Work. Each protocol page is a folder within one of these groups. For consistency it should be named in Capitalized_Snakecase and contain a Quarto document called index.qmd. There may be images in this folder as well. See the documentation on preparing protocols.\nprotocols\n├── Drones\n│   └── Pix4Dmapper_Stitch\n│       ├── Picture1.png\n│       ├── ...\n│       ├── Picture8.png\n│       └── index.qmd\n├── Logistics\n└── Operations"
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#making-new-protocols",
    "href": "protocols/Operations/Website_Overview/index.html#making-new-protocols",
    "title": "Website Overview",
    "section": "Making new protocols",
    "text": "Making new protocols\nThe easiest way to make a new protocol is to copy the folder for an existing protocol and modify that. Give the folder an informative, Capitalized_Snakecase name and delete any photos or other artifacts that you don’t need. Then open up the index.qmd file and begin editing it.\nAt the top of the file you’ll see some YAML which provides metadata for the page.\n---\ntitle: \"Building an RStudio Singularity Containers\"\nauthor: \"Daniel Kick\"\ndate: \"5/31/2023\"\ndate-modified: \"5/31/2023\"\nexecute:\n  freeze: true\n---\nNote the last two lines here. At present this document is frozen so it will not be re-rendered when the website is built. This means that to update this page you’ll need to click the “Render” button on it before rendering the website. Any pages with code that rely on non-standard libraries should be frozen (so that other people don’t need those installed to rebuild the site) but it is okay to remove these lines while working on a protocol."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#making-protocols-visible",
    "href": "protocols/Operations/Website_Overview/index.html#making-protocols-visible",
    "title": "Website Overview",
    "section": "Making protocols visible",
    "text": "Making protocols visible\nOnce a protocol is added it needs to be linked on the index.qmd in the project root so it is accessible. Open that document and add a link to the html version of the file. For example, this page is linked on that page as [Website Overview](/protocols/Operations/Website_Overview/index.html). Once this is done, rebuild the website by clicking ther “Render Website” button under the “Build” tab."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#deploying-updates",
    "href": "protocols/Operations/Website_Overview/index.html#deploying-updates",
    "title": "Website Overview",
    "section": "Deploying updates",
    "text": "Deploying updates\n&lt;font color =“red”&gt; *This section is intentionally left blank. Likely we will deploy through GitHub pages* &lt;/font&gt;"
  },
  {
    "objectID": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html",
    "href": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html",
    "title": "Using your Singularity Container with Open OnDemand",
    "section": "",
    "text": "In this how-to we’ll make a singularity container accessible through Open OnDemand on Atlas. This allows for code on the HPC to be run in the same environment used for development locally. To do this, we’ll specify a new jupyter kernel that Open OnDemand will connect to."
  },
  {
    "objectID": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#setup",
    "href": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#setup",
    "title": "Using your Singularity Container with Open OnDemand",
    "section": "Setup",
    "text": "Setup\nThis document assumes you have a pre-built Singularity container (.sif) on Atlas. Using a custom container is recommended, but there is at least one container on Atlas that could be used (e.g. /reference/containers/jupyter-tensorflow/jupyter-tensorflow-2.2.0.sif). Note that your container should include Jupyter.\n\nCreate a symbolic link between your home directory and the project data directory.\n\ncd ~ \nln -s /projects/the_labs_project/your_directory\n\nCreate a directory to hold your container(s)\n\nmkdir ipython_containers\n\nUse Globus to transfer your container to this folder. In this example, my container is called tensorflow-21.07-tf2-py3.sif.\nSetup a kernel for your container. This will create configuration files that we will customize. They will be in a folder in .local/share/jupyter/kernels/.\n\nmodule load python\npython3 -m ipykernel install --user --name singularity-tf2py3 --display-name \"Python 3 Tensorflow 2\"\n\nCustomize kernel configuration. Open the new kernel configuration json in in your preferred text editor.\n\nvim .local/share/jupyter/kernels/singularity-tf2py3/kernel.json\n\nEdit the existing file so that it looks like the below. Be sure to edit the container path so that it has your container and username and the display_name so that your container is recognizable.\n\n\n{\n \"argv\": [\n  \"/apps/singularity-3/singularity-ce-3.11.0/bin/singularity\",\n  \"exec\",\n  \"--nv\",\n  \"-B\",\n  \"/project\",\n  \"-B\",\n  \"/90daydata\",\n  \"-B\",\n  \"/reference\",\n  \"-B\",\n  \"/local/scratch\",\n  \"/home/user.name/ipykernel_containers/tensorflow-21.07-tf2-py3.sif\",\n  \"/usr/bin/python3\",\n  \"-m\",\n  \"ipykernel\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"interrupt_mode\": \"message\",\n \"display_name\": \"Python 3 Tensorflow 2 Singularity\",\n \"language\": \"python\"\n}"
  },
  {
    "objectID": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#usage",
    "href": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#usage",
    "title": "Using your Singularity Container with Open OnDemand",
    "section": "Usage",
    "text": "Usage\n\nLog into Open OnDemand and begin a Jupyter session.\nSelect “Kernel” and “Change kernel”. Select your newly defined kernel.\n\nOnce the kernel starts all your code will be executed in the container!"
  },
  {
    "objectID": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#other-considerations",
    "href": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#other-considerations",
    "title": "Using your Singularity Container with Open OnDemand",
    "section": "Other Considerations",
    "text": "Other Considerations\nSome containers (see below) use a python other than /usr/bin/python3 as the default python. This will cause ipykernel to not be found when executed. The solution is to check the location of the default python in the container like so:\nsingularity exec container.sif which python\nNote the location and correct the path in kernel.json. In the case of the container created with the below .def file the path is /opt/conda/bin/python.\nBootstrap: docker\nFrom: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime\n\n%post\n    apt-get update\n    apt-get upgrade -y\n    ldconfig\n    apt-get install -y python3-pip\n    pip3 install --upgrade pip\n    echo 'ipykernel==6.15.1\nipython==8.4.0\nipython-genutils==0.2.0\nipywidgets==7.7.1\njupyter==1.0.0\njupyter-client==7.3.4\njupyter-console==6.4.4\njupyter-core==4.11.1\njupyter-server==1.18.1\njupyterlab==3.4.4\njupyterlab-pygments==0.2.2\njupyterlab-server==2.15.0\njupyterlab-widgets==1.1.1\nnbclassic==0.4.3\nnbclient==0.6.6\nnbconvert==6.5.0\nnbformat==5.4.0\nnotebook==6.4.12' &gt; requirements.txt\n    pip3 install -r requirements.txt"
  },
  {
    "objectID": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#references",
    "href": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#references",
    "title": "Using your Singularity Container with Open OnDemand",
    "section": "References",
    "text": "References\nThis was inspired by this MSI article and done with the help of James Huston Rogers at MSU. For cases where port forwarding is allowed (remote machines and some HPCs ) that will easier."
  },
  {
    "objectID": "protocols/Logistics/HPC_Moving_Data/index.html",
    "href": "protocols/Logistics/HPC_Moving_Data/index.html",
    "title": "Moving Data from Computer to Computer",
    "section": "",
    "text": "Globus is the recommended way to move data from your local computer to the cluster. It is a service run from uchicago and allows you to move data using a web browser. Refer to SciNet’s install instructions.\nOnce installed you’ll need to link any remote storage. Atlas’ collection name is msuhpc2#Atlas-dtn. Linking it will require logging in with  with your Ceres password. \n\nOnce linked, you’ll be able to move data between linked drives."
  },
  {
    "objectID": "protocols/Logistics/HPC_Moving_Data/index.html#globus",
    "href": "protocols/Logistics/HPC_Moving_Data/index.html#globus",
    "title": "Moving Data from Computer to Computer",
    "section": "",
    "text": "Globus is the recommended way to move data from your local computer to the cluster. It is a service run from uchicago and allows you to move data using a web browser. Refer to SciNet’s install instructions.\nOnce installed you’ll need to link any remote storage. Atlas’ collection name is msuhpc2#Atlas-dtn. Linking it will require logging in with  with your Ceres password. \n\nOnce linked, you’ll be able to move data between linked drives."
  },
  {
    "objectID": "protocols/Logistics/HPC_Moving_Data/index.html#rsync-and-scp",
    "href": "protocols/Logistics/HPC_Moving_Data/index.html#rsync-and-scp",
    "title": "Moving Data from Computer to Computer",
    "section": "Rsync and scp",
    "text": "Rsync and scp\nCommand line tools like rsync and scp can be used to move files between workstations in the lab or to a cluster. To move data to or from Atlas one would use the following and then provide your password and authentication code.\nscp ./file &lt;SCINet UserID&gt;@Atlas-dtn.hpc.msstate.edu:/path/to/destination\nNote that the computer you’re connecting to is Atlas-dtn.hpc.msstate.edu instead of Atlas-Login.hpc.msstate.edu. This will access a data transfer node leaving the login nodes free for others to use.\nIf you need to move a directory, use the recursive flag -r or use rsync. With the latter this might look like:\nrsync -azv --progress ./file &lt;SCINet UserID&gt;@Atlas-dtn.hpc.msstate.edu:/path/to/destination\nThe options here ensure the file permissions are maintained (-a archive), files are zipped before transfer (-z zip), information is written to standard output (-v verbose), and transfer progress is provided (--progress)."
  },
  {
    "objectID": "protocols/Logistics/Do_Nothing_Scripting/index.html",
    "href": "protocols/Logistics/Do_Nothing_Scripting/index.html",
    "title": "Working Towards Automation: Do Nothing Scripting",
    "section": "",
    "text": "“Do Nothing Scripting” is a great practice. In brief the idea is to begin with a script that prints the instructions for a task but does not execute the instructions. Rahter it waits until the user presses a key or otherwise confirms that the task has been completed. Think of it like a check list – it contains the instructions and will display them as you complete them.\nIn our lab the main languages we use are python, R, and bash. In each of these languages the following functions would suspend the script until the user is ready to proceed.\n# python\ndef wait_for_enter():\n    input(\"Press a key to continue.\")\n    \nwait_for_enter()\n# R\nwait_for_enter &lt;- function(){\n  readline(prompt = \"Press a key to continue.\")\n}\n\nwait_for_enter()\n# bash\nwait_for_enter(){\n echo \"Press a key to continue.\"\n read -n 1 INVALUE\n}\n\nwait_for_enter\nDo nothing scripting can allow for steps that may change to be dynamically updated as in this example of in the rootbot sorting script. The jupyter notebook prints out instructions for the user to complete externally.\nprint(\"\"\"Instructions:\n1. Go to \"\"\"+path_base+'Experiments/'+experiment+\"\"\"\n2. Run rsync -azv --files-from=./send_files.txt ./ ../dest/\n   or if remote -azv --files-from:=./send_files.txt ./ ../dest/\n\"\"\")\nIt also allows for steps in that process to be automated over time. A step can begin as a list of instructions and ultimately be replaced by a function or external script."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_PortForward/index.html",
    "href": "protocols/Logistics/Container_Singularity_PortForward/index.html",
    "title": "Testing (and using) your Singularity Container with Port Forwarding",
    "section": "",
    "text": "Note: this how to guide assumes that you have a working container with jupyter installed. See this page for details on setting up the gpu.sif container used here.\nThere are two steps to access jupyter remotely. First, start jupyter in the container. This can be done like so\nsingularity exec --nv gpu.sif jupyter notebook --no-browser --port=8887\nor can be started within a shell in the container if desired.\nuser.name$ singularity shell --nv gpu.sif\nSingularity&gt; jupyter notebook --no-browser --port=8887\nNote that here the --nv flag is not needed if gpu access is not desired. Further we’re specifying a port to be used for the notebook.\nIf the above is being run on a remote machine that allows port forwarding then we can open a ssh session from the local machine to connect the remote port (8887) to a local port. Here we use the same port number but if that one is busy another could be used.\nssh -N -f -Y -L 8887:localhost:8887 labmember\\@10.206.28.81\nAfter this session is active, then you can interact with jupyter as you normally would."
  },
  {
    "objectID": "protocols/DryLab/Rovers/RoverUseInField/index.html",
    "href": "protocols/DryLab/Rovers/RoverUseInField/index.html",
    "title": "Operating Rovers in the Field",
    "section": "",
    "text": "How to Start a Mission in the Field\nBattery Installation\n\nWARNING: LiPo batteries are extremely dangerous! Before using LiPo batteries, make sure you have read the Battery procedures SOP and been trained.\nRemove the Rover from its travel case.\nOpen the compartment on the top of the rover by sliding it over.\nThere is a wire connector inside of the battery bay, as well as a digital battery monitor.\nConnect the battery to the battery monitor and review what is displayed on the digital reader\nYou should plug the four prongs into the far left side of the digital monitor (there are 4 holes)\n\n\n\nIt should display a total combined voltage of ~16.8V and display the voltage contained in each cell, ~4.2V each.\nIf any of the cells are below there is a difference of greater than\n\n0.3 between cells, do not use this battery and instead use a different one.\n\nPlace the messed-up battery in its fireproof case, log it in your notebook, and report it to Harper or Jacob.\nThe Rover contains a silver button on its aft side. Press this button to boot it up.\nIt will take 5-10 minutes for the rover to start up; when it is nearly started, you will hear a fan turn on inside of it.\nWatch the tablet loading screen; it should display a rover symbol in the upper right hand of the screen when she is ready. If you do not see this or hear the fan turn on, reboot the Rover by pressing the silver button.\n\nBeginning Mission\n1. As the rover is powering up, record your name, date and time, and battery number in the notebook.\n\nManually drive the rover to the beginning point of the mission (first or last row)\nWhen you are at the starting point, go to mission on the tablet and select the field you will be driving through.\nWhen you are ready, press the record button and select the correct row and column you will be traveling down.\nThe rover will automatically begin driving on its own; attempt to use autopilot as much as possible (this may be difficult when there are gaps or passing through rows as it tends to veer off on its own)\nWhen not using autopilot, use the sliders to control the direction and speed of rover (it is not a race, drive it slowly).\nContinue down the row until you come into contact with the orange flag at the end. At this point, press the record button again to collect the data from that row by selecting the same row, but different column.\nJump to the next row by driving manually (skipping a “lane”); there should not be a flag where you begin.\nPress the record button again and select the correct row and column (new row, same column) and repeat 4 down\n\n• Some times mislabeling of recordings occurs. If this happens, proceed as normal, but write it down in the notebook to make the correction during data upload back at the lab.\n• Any issues you have with the Rover take note of it.\n• You will likely have to change the battery during the mission. You will know you need to change the battery when you hear a loud beeping noise. When the noise goes off, finish the row you are on and power down Natasha at the end of the row (after you collect/save the recorded data).\no Replace the battery with a new one; place used battery in fireproof bag. Reboot Natasha (takes 5-10 minutes), continue mission\no Log where you stopped and started again in the log book\n• Occasionally the wifi between Natasha and the tablet will disconnect and you cannot control her. Wait 2 minutes to see if she reconnects – this is faster than a reboot. Simply turn her off and reboot her. Record this in the log; you will likely need to redo the row.\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "protocols/DryLab/Rovers/RoverUseInField/index.html#instructions",
    "href": "protocols/DryLab/Rovers/RoverUseInField/index.html#instructions",
    "title": "Operating Rovers in the Field",
    "section": "",
    "text": "How to Start a Mission in the Field\nBattery Installation\n\nWARNING: LiPo batteries are extremely dangerous! Before using LiPo batteries, make sure you have read the Battery procedures SOP and been trained.\nRemove the Rover from its travel case.\nOpen the compartment on the top of the rover by sliding it over.\nThere is a wire connector inside of the battery bay, as well as a digital battery monitor.\nConnect the battery to the battery monitor and review what is displayed on the digital reader\nYou should plug the four prongs into the far left side of the digital monitor (there are 4 holes)\n\n\n\nIt should display a total combined voltage of ~16.8V and display the voltage contained in each cell, ~4.2V each.\nIf any of the cells are below there is a difference of greater than\n\n0.3 between cells, do not use this battery and instead use a different one.\n\nPlace the messed-up battery in its fireproof case, log it in your notebook, and report it to Harper or Jacob.\nThe Rover contains a silver button on its aft side. Press this button to boot it up.\nIt will take 5-10 minutes for the rover to start up; when it is nearly started, you will hear a fan turn on inside of it.\nWatch the tablet loading screen; it should display a rover symbol in the upper right hand of the screen when she is ready. If you do not see this or hear the fan turn on, reboot the Rover by pressing the silver button.\n\nBeginning Mission\n1. As the rover is powering up, record your name, date and time, and battery number in the notebook.\n\nManually drive the rover to the beginning point of the mission (first or last row)\nWhen you are at the starting point, go to mission on the tablet and select the field you will be driving through.\nWhen you are ready, press the record button and select the correct row and column you will be traveling down.\nThe rover will automatically begin driving on its own; attempt to use autopilot as much as possible (this may be difficult when there are gaps or passing through rows as it tends to veer off on its own)\nWhen not using autopilot, use the sliders to control the direction and speed of rover (it is not a race, drive it slowly).\nContinue down the row until you come into contact with the orange flag at the end. At this point, press the record button again to collect the data from that row by selecting the same row, but different column.\nJump to the next row by driving manually (skipping a “lane”); there should not be a flag where you begin.\nPress the record button again and select the correct row and column (new row, same column) and repeat 4 down\n\n• Some times mislabeling of recordings occurs. If this happens, proceed as normal, but write it down in the notebook to make the correction during data upload back at the lab.\n• Any issues you have with the Rover take note of it.\n• You will likely have to change the battery during the mission. You will know you need to change the battery when you hear a loud beeping noise. When the noise goes off, finish the row you are on and power down Natasha at the end of the row (after you collect/save the recorded data).\no Replace the battery with a new one; place used battery in fireproof bag. Reboot Natasha (takes 5-10 minutes), continue mission\no Log where you stopped and started again in the log book\n• Occasionally the wifi between Natasha and the tablet will disconnect and you cannot control her. Wait 2 minutes to see if she reconnects – this is faster than a reboot. Simply turn her off and reboot her. Record this in the log; you will likely need to redo the row.\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "protocols/Drones/QGIS_First_Grid/index.html",
    "href": "protocols/Drones/QGIS_First_Grid/index.html",
    "title": "Gridding the Reference Flight",
    "section": "",
    "text": "Open QGIS and open a “New Project”\nGo to the “Layer” tab at the top, then “Add Raster Layer” - where it says browse, go to the Pix4D folder for the date and field being gridded and find the orthomosaic\nWhen the orthomosaic shows up on the screen, go to the “Rotation” bar in the bottom right corner and change the rotation angle until the orthomosaic is straight (As straight as it can be, doesn’t have to be perfect)\nTo create the origin point where the grid will start from:\n\nNavigate to the “Layer” tab at the top of toolbar then “Create Layer” then “New Shapefile Layer”\nSave with the name “origin”\nClick the “Toggle Editing” and then “Add Feature” - make the point in the lower left corner above the GCP in between the plants in the first experimental plot\nWhen the labeling window comes up, name the point “1”\n\nTo get to the processing toolbox:\n\nGo to “Plugins” then “Manage and Install Plugins”\nRe-check the box for “Processing”\nClose the tab and click on the “Setting” icon that looks like a gear and the toolbox will appear\n\nClick on the “R” option and in the drop down menu click “Draw Plots from points” - a window should appear with all of the options for inputting the range and row measurements. To create the grid:\n\nBased on information from the field map given, enter the number of ranges in the box for “Number of ranges per block” and enter the number of rows in the box for “Number of rows per block”\nIn the angle of rotation, enter the opposite (+/-) number of the rotation the orthomosaic is set at, which can be seen in the bottom right corner of the screen\nMake the measurement unit for plot size “feet” and the ID format “serpentine”\nStart the numbering plots should be “bottom left”\nThe “Full plot height” and “Data plot height should be the length of each plot in feet and the full plot width and data plot width should be the width of each plot in feet\nSet the Field containing ID to “123 id” (a common error with the script happens when this setting isn’t correct)\n\nGo to the log tab at the top of the window to see if the script runs successfully\nWhen the script is finished running, the grid will generate on top of the orthomosaic with the boxes colored in. To make the boxes transparent:\n\nClick on the layer in the “Layers” Menu on the left side of the screen\nIn the dropdown menu, go to “Properties” then “Symbology” on the left side of the window\nChange the fill to the red outline\nClick on the “Simple Line” below “Fill” and change the line width to “Hairline”\nClick “Ok”\n\nGo through each row of the grid checking that each box contains one plot - the plots on the end of each row and range don’t need a box because they’re border - so when the grid is used for future flights the plots will, for the most part, already be lined up (This step will probably take the longest)"
  },
  {
    "objectID": "protocols/Drones/QGIS_First_Grid/index.html#instructions",
    "href": "protocols/Drones/QGIS_First_Grid/index.html#instructions",
    "title": "Gridding the Reference Flight",
    "section": "",
    "text": "Open QGIS and open a “New Project”\nGo to the “Layer” tab at the top, then “Add Raster Layer” - where it says browse, go to the Pix4D folder for the date and field being gridded and find the orthomosaic\nWhen the orthomosaic shows up on the screen, go to the “Rotation” bar in the bottom right corner and change the rotation angle until the orthomosaic is straight (As straight as it can be, doesn’t have to be perfect)\nTo create the origin point where the grid will start from:\n\nNavigate to the “Layer” tab at the top of toolbar then “Create Layer” then “New Shapefile Layer”\nSave with the name “origin”\nClick the “Toggle Editing” and then “Add Feature” - make the point in the lower left corner above the GCP in between the plants in the first experimental plot\nWhen the labeling window comes up, name the point “1”\n\nTo get to the processing toolbox:\n\nGo to “Plugins” then “Manage and Install Plugins”\nRe-check the box for “Processing”\nClose the tab and click on the “Setting” icon that looks like a gear and the toolbox will appear\n\nClick on the “R” option and in the drop down menu click “Draw Plots from points” - a window should appear with all of the options for inputting the range and row measurements. To create the grid:\n\nBased on information from the field map given, enter the number of ranges in the box for “Number of ranges per block” and enter the number of rows in the box for “Number of rows per block”\nIn the angle of rotation, enter the opposite (+/-) number of the rotation the orthomosaic is set at, which can be seen in the bottom right corner of the screen\nMake the measurement unit for plot size “feet” and the ID format “serpentine”\nStart the numbering plots should be “bottom left”\nThe “Full plot height” and “Data plot height should be the length of each plot in feet and the full plot width and data plot width should be the width of each plot in feet\nSet the Field containing ID to “123 id” (a common error with the script happens when this setting isn’t correct)\n\nGo to the log tab at the top of the window to see if the script runs successfully\nWhen the script is finished running, the grid will generate on top of the orthomosaic with the boxes colored in. To make the boxes transparent:\n\nClick on the layer in the “Layers” Menu on the left side of the screen\nIn the dropdown menu, go to “Properties” then “Symbology” on the left side of the window\nChange the fill to the red outline\nClick on the “Simple Line” below “Fill” and change the line width to “Hairline”\nClick “Ok”\n\nGo through each row of the grid checking that each box contains one plot - the plots on the end of each row and range don’t need a box because they’re border - so when the grid is used for future flights the plots will, for the most part, already be lined up (This step will probably take the longest)"
  },
  {
    "objectID": "protocols/Drones/Pix4Dmapper_Stitch/index.html",
    "href": "protocols/Drones/Pix4Dmapper_Stitch/index.html",
    "title": "Stitching with Pix4Dmapper",
    "section": "",
    "text": "Pix4Dmapper is installed on the lambda2 in the dry lab (NOT the Linux lambda)\n\nPassword: █████████-███-███████-██████ (You put it in your password wallet right?😉)\n\nCheck what flights need to be stitched on the To Be Stitched List_2022: Teams &gt; UAV Missions &gt; Files\nLaunch Pix4Dmapper on desktop\nSelect New Project\n\nName file FlightDate(YYMMDD)_camera(m2pro/ANAFI)_FieldName_Pix4D\nCreate In: This PC &gt; Desktop &gt; Pix4D &gt; 202X Fields &gt; “Your Field Name” &gt; 1 Folder called FlightDate(YYMMDD)_drone(m2pro/RE-mx/ANAFI)_FieldName_Pix4D\n\nSelect Images, Click the Add Images… button, file explorer will open.\n\nNavigate to This PC &gt; wldata (Under Network locations) &gt; Field_Data_202X &gt; UAV_images_by_field_202X &gt; Select a Field &gt; Select a folder by Date (YYMMDD) and drone type (m2pro/ANAFI only)\nCtrl + A to select all of the JPGs in the folder, Click Open (images will be selected, green check will appear) then Click Next &gt;\n\nKeep default Image Properties (Pix4D uses the GPS info in drone images), Click Next &gt;\nKeep default settings of Select Output Coordinate System, Click Next &gt;\nProcessing Options Template will open, under Standard select 3D Maps, (Do not check the box next to Start Processing), Click Finish\nAfter the map loads, Import the GCPs\n\nSelect the Project tab\nSelect GCP/MTP Manager…, the GCP Manager Window will open\nIn the GCP Coordinate System click the Edit… button, the Select GCP Coordinate System window will open\n\nCheck the Advanced Coordinate Options box\nClick the From List… button under the Known Coordinate System [m] search bar, Coordinate System window will open\nSelect the following from the dropdown lists\n\nDatum: WGS 1984\nCoordinate System: WGS 84 (Top of list, look for the globe )\n\n\n\nIn the GCP/MTP Table click the Import GCPs… button, the Import Ground Control Points window will open\n\nCoordinates order: Longitude, Latitude, Altitude\nClick Browse…, navigate to This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; GCPs &gt; Select your field’s .txt file, Click Open, Click OK, Click OK. The GCPs will appear as blue crosses on the map\nSave Project!\n\n\nInitial Processing\n\nClick the Processing tab on the bottom left side of the window, check the box next to 1. Initial Processing (ensure other boxes are unchecked) )\nClick the Processing Options tab on the bottom left side of the window (gear shape icon), the Processing Options window will open, ensure Initial Processing step is clicked on\n\nClick the Matching tab, in the Matching Strategy header check the Use Geometrically Verified Matching box )\nClick the Calibration tab, under the Camera Optimization header use the Internal Parameters Optimization dropdown box and select All Prior, Click OK, window will close )\nAt the bottom of the screen Click the Start bottom to begin Initial Processing (This will take hours, just leave Pix4D running 😊) )\n\n\nMarking GCPs\n\nAfter initial processing switch to rayCloud view on the left side of the Pix4D Window, Click on the blue GCP marker and Images will open up on the right (I like to uncheck Cameras and Rays for a cleaner map) )\nYou can press hold down to move around the images and zoom in and out to find the GCP (they may not be in every image, that’s ok)\nClick on the center of the GCP to mark it, repeat on 8-15 images. Do this for each GCP.\n\nIn the Selection panel above the Image panel, Click the Apply button occasionally to update and mark the GCPs\nSelect the Process tab at the top of the window\n\nSelect Reoptimize (this will take about 10 minutes), a Warning will come up, Click OK\nGenerate a newQuality Report by Clicking the Process tab and Selecting Generate Quality Report (this will take about 15 minutes). Ensure there are green checks next to the 5 parameters in the Quality Check (if not, troubleshoot w/ help from the Pix4D website)\n\nSave Project!\n\nPoint Cloud and Mesh and DSM, Orthomosaic and Index\n\nCheck the box next to 2. Point Cloud and Mesh and 3. DSM, Orthomosaic and Index, in the processing window (be sure to uncheck the previous step so Pix4D doesn’t rerun and take even longer), Click Start and accept default Processing Options (this will take hours, leave Pix4D running 😊)\nSave Project!\nTo see your 3D map, in the Ray Cloud viewer check the box next to Point Graphs (let it load) and then check the box next to Triangle Meshes. Enjoy!\nUpdate the To Be Stitched List_202X on Teams &gt; UAV Missions &gt; Files\n\nOpen orthomosaic in QGIS.\n\nCreate a directory within the flight directory with the name of the flight directory plus “_QGIS.”\n\nOpen QGIS\nSave project in newly created QGIS directory with flight name plus “_QGIS”.\nGo to Layer -&gt; Add Layer -&gt; Add Raster Layer.\nFind the orthomosaic “.tif” file just created by Metashape. And add it.\nGo to Project -&gt; Properties. Select CRS from the side tab. Change the Coordinate Reference System to “WGS 84 / UTM zone 15N” Authority ID: “EPSG:32615” if needed.\nRotate the image using the controls on the bottom taskbar if needed. Rows should be vertical and ranges horizontal.\n\n\nImport reference grid.\n\nCopy the Reference_grid file from the main field directory to the QGIS directory.\nRename the copied file replacing the field name and year with the flight name. (e.g. “Reference_grid_210717_m2pro_Gen7.gpkg”\nGo to Layer -&gt; Add Layer -&gt; Add Vector Layer.\nSelect the new Reference_Grid file you just renamed and click Open. Then click Add. Then Close.\nRight Click on the newly added layer and go to Properties. Select Symbology in the left hand pane. Select Simple Fill. Change the Fill Color to Transparent Fill. Click ok.\nCheck that the grid lines up properly with the field data. Make any minor adjustments that may be needed."
  },
  {
    "objectID": "protocols/Drones/Pix4Dmapper_Stitch/index.html#instructions",
    "href": "protocols/Drones/Pix4Dmapper_Stitch/index.html#instructions",
    "title": "Stitching with Pix4Dmapper",
    "section": "",
    "text": "Pix4Dmapper is installed on the lambda2 in the dry lab (NOT the Linux lambda)\n\nPassword: █████████-███-███████-██████ (You put it in your password wallet right?😉)\n\nCheck what flights need to be stitched on the To Be Stitched List_2022: Teams &gt; UAV Missions &gt; Files\nLaunch Pix4Dmapper on desktop\nSelect New Project\n\nName file FlightDate(YYMMDD)_camera(m2pro/ANAFI)_FieldName_Pix4D\nCreate In: This PC &gt; Desktop &gt; Pix4D &gt; 202X Fields &gt; “Your Field Name” &gt; 1 Folder called FlightDate(YYMMDD)_drone(m2pro/RE-mx/ANAFI)_FieldName_Pix4D\n\nSelect Images, Click the Add Images… button, file explorer will open.\n\nNavigate to This PC &gt; wldata (Under Network locations) &gt; Field_Data_202X &gt; UAV_images_by_field_202X &gt; Select a Field &gt; Select a folder by Date (YYMMDD) and drone type (m2pro/ANAFI only)\nCtrl + A to select all of the JPGs in the folder, Click Open (images will be selected, green check will appear) then Click Next &gt;\n\nKeep default Image Properties (Pix4D uses the GPS info in drone images), Click Next &gt;\nKeep default settings of Select Output Coordinate System, Click Next &gt;\nProcessing Options Template will open, under Standard select 3D Maps, (Do not check the box next to Start Processing), Click Finish\nAfter the map loads, Import the GCPs\n\nSelect the Project tab\nSelect GCP/MTP Manager…, the GCP Manager Window will open\nIn the GCP Coordinate System click the Edit… button, the Select GCP Coordinate System window will open\n\nCheck the Advanced Coordinate Options box\nClick the From List… button under the Known Coordinate System [m] search bar, Coordinate System window will open\nSelect the following from the dropdown lists\n\nDatum: WGS 1984\nCoordinate System: WGS 84 (Top of list, look for the globe )\n\n\n\nIn the GCP/MTP Table click the Import GCPs… button, the Import Ground Control Points window will open\n\nCoordinates order: Longitude, Latitude, Altitude\nClick Browse…, navigate to This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; GCPs &gt; Select your field’s .txt file, Click Open, Click OK, Click OK. The GCPs will appear as blue crosses on the map\nSave Project!\n\n\nInitial Processing\n\nClick the Processing tab on the bottom left side of the window, check the box next to 1. Initial Processing (ensure other boxes are unchecked) )\nClick the Processing Options tab on the bottom left side of the window (gear shape icon), the Processing Options window will open, ensure Initial Processing step is clicked on\n\nClick the Matching tab, in the Matching Strategy header check the Use Geometrically Verified Matching box )\nClick the Calibration tab, under the Camera Optimization header use the Internal Parameters Optimization dropdown box and select All Prior, Click OK, window will close )\nAt the bottom of the screen Click the Start bottom to begin Initial Processing (This will take hours, just leave Pix4D running 😊) )\n\n\nMarking GCPs\n\nAfter initial processing switch to rayCloud view on the left side of the Pix4D Window, Click on the blue GCP marker and Images will open up on the right (I like to uncheck Cameras and Rays for a cleaner map) )\nYou can press hold down to move around the images and zoom in and out to find the GCP (they may not be in every image, that’s ok)\nClick on the center of the GCP to mark it, repeat on 8-15 images. Do this for each GCP.\n\nIn the Selection panel above the Image panel, Click the Apply button occasionally to update and mark the GCPs\nSelect the Process tab at the top of the window\n\nSelect Reoptimize (this will take about 10 minutes), a Warning will come up, Click OK\nGenerate a newQuality Report by Clicking the Process tab and Selecting Generate Quality Report (this will take about 15 minutes). Ensure there are green checks next to the 5 parameters in the Quality Check (if not, troubleshoot w/ help from the Pix4D website)\n\nSave Project!\n\nPoint Cloud and Mesh and DSM, Orthomosaic and Index\n\nCheck the box next to 2. Point Cloud and Mesh and 3. DSM, Orthomosaic and Index, in the processing window (be sure to uncheck the previous step so Pix4D doesn’t rerun and take even longer), Click Start and accept default Processing Options (this will take hours, leave Pix4D running 😊)\nSave Project!\nTo see your 3D map, in the Ray Cloud viewer check the box next to Point Graphs (let it load) and then check the box next to Triangle Meshes. Enjoy!\nUpdate the To Be Stitched List_202X on Teams &gt; UAV Missions &gt; Files\n\nOpen orthomosaic in QGIS.\n\nCreate a directory within the flight directory with the name of the flight directory plus “_QGIS.”\n\nOpen QGIS\nSave project in newly created QGIS directory with flight name plus “_QGIS”.\nGo to Layer -&gt; Add Layer -&gt; Add Raster Layer.\nFind the orthomosaic “.tif” file just created by Metashape. And add it.\nGo to Project -&gt; Properties. Select CRS from the side tab. Change the Coordinate Reference System to “WGS 84 / UTM zone 15N” Authority ID: “EPSG:32615” if needed.\nRotate the image using the controls on the bottom taskbar if needed. Rows should be vertical and ranges horizontal.\n\n\nImport reference grid.\n\nCopy the Reference_grid file from the main field directory to the QGIS directory.\nRename the copied file replacing the field name and year with the flight name. (e.g. “Reference_grid_210717_m2pro_Gen7.gpkg”\nGo to Layer -&gt; Add Layer -&gt; Add Vector Layer.\nSelect the new Reference_Grid file you just renamed and click Open. Then click Add. Then Close.\nRight Click on the newly added layer and go to Properties. Select Symbology in the left hand pane. Select Simple Fill. Change the Fill Color to Transparent Fill. Click ok.\nCheck that the grid lines up properly with the field data. Make any minor adjustments that may be needed."
  },
  {
    "objectID": "posts/DanielKick/index.html",
    "href": "posts/DanielKick/index.html",
    "title": "Writings",
    "section": "",
    "text": "Worse is better and not doing things “right”\n\n\n\ntacit knowledge\n\n\nbeginner\n\n\nintermediate\n\n\ncode\n\n\ndeep learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse a spreadsheet to manage your CV & Resume\n\n\n\nprofessional development\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation as a Super Power\n\n\n\ncode\n\n\nintermediate\n\n\nensembling\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the Wrong Problem\n\n\n\ncode\n\n\ndebugging\n\n\ntacit knowledge\n\n\nintermediate\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a “Visible” Neural Network\n\n\n\ncode\n\n\nadvanced\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSave only what you need\n\n\n\ncode\n\n\ndebugging\n\n\ntacit knowledge\n\n\nbeginner\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing Tacit Knowledge Through Blogging\n\n\n\ntacit knowledge\n\n\nbeginner\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Do-nothing Scripting in Bash\n\n\n\ncode\n\n\nbash\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia: R can have Comments in Tables\n\n\n\ncode\n\n\nr\n\n\nintermediate\n\n\ntips\n\n\ntrivia\n\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Open a new Interactive Session in Running Session\n\n\n\ncode\n\n\nhpc\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia: In Python Missing Isn’t Equal to Itself\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\ntrivia\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Cache Intermediate Results with pickle\n\n\n\ncode\n\n\npython\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: For those coming from R: Silent In Place Replacement\n\n\n\ncode\n\n\npython\n\n\nr\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Reusing Custom Functions\n\n\n\ncode\n\n\npython\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: More Readable Data with pretty-print\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Find the Graph you want in using a Graph Gallery\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Jupyter Plugins\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 9, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "",
    "text": "For the past year I’ve been using a spreadsheet and RStudio to manage my resume and curriculum vitae. This post is a pitch for why you might want to do this and an overview of the system. There will be a follow up post on how to get started if you decide to use his approach."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#why-use-a-spreadsheet",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#why-use-a-spreadsheet",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Why use a spreadsheet?",
    "text": "Why use a spreadsheet?\nThe key reason why you should use a spreadsheet to manage your resume is that it enables easy filtering and sorting. In essence your spreadsheet becomes a personal work history database from which you can quickly retrieve entries relevant to the application at hand.\nIf instead all your experiences went into an all encompassing text document, tailoring a document to a position would require you to go through each section and cut out most of the entries. To be clear – there are far worse strategies out there. Having a single reference document keeps your information together and means that much of your formatting work is done ahead of time.\nThis is where RStudio comes in. Rmarkdown gives you a way to draw entries from your spreadsheet, filter them, and then turn those entries into beautifully formatted text. Not to mention that you can have R update text for you1."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#overview",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#overview",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Overview",
    "text": "Overview\nLet’s see how this works. All the code and data for my resume and cv is on GitHub. Every time I update the spreadsheet or tweak the documents’ aesthetics I add a commit.\n\nUpdating the documents is a 4 step process that takes just a few moments. Earlier this month when a paper of mine was accepted. To update my documents I…\n\nedited the positions.csv document and added a row with the paper’s bibliographic information and url.\nopened my Rmarkdown files Curriculum-Vitae.Rmd and Resume.Rmd and knit them to pdfs.\ncopied the output pdfs to a second repository to share with others\ncommited and pushed these new documents to GitHub (and then did the same for the cv repository)\n\n\nYou might be wondering – what’s with the last two steps? Why move the documents and then push to GitHub instead of keeping them where they are? And why put these on GitHub in the first place?\nPutting these on GitHub (or online for that matter) makes it easy to go from your resume to your online presence (LinkedIn, Orcid, personal website, etc.) from links in your resume. This means that if you embed a link to your resume in your resume then every paper copy you hand out and every business card links to the most up to date version.\n\nUsing a second repository is just to have a cleaner presentation. Every file that isn’t your resume is a distraction – and if your recipient wants to see your GitHub they’ll be just one click away."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#a-more-detailed-look",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#a-more-detailed-look",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "A More Detailed Look:",
    "text": "A More Detailed Look:\nLet’s take a look at positions.csv.\nThe first column, section, is the categories in your document. Several categories might be presented together (e.g. national_presentations and regional_presentations) but they aren’t assumed to be. The next column, in_resume, is a simple filter. Everything goes into the curriculum vitae, not so with the resume. Next we have institution. This one is a little odd because it includes university or organization names but also lists of authors (e.g. row 44). This is because all the items in this column are formatted the same way. After showing the title of an entry, we want to show this information. Dates are included using the start and end columns and location information (here urls) in loc. Finally, are several description_ columns. Extra information you want can be added to these.\n\nWhen I updated this spreadsheet I already had an entry from when I submitted the paper and put it $bioR\\chiiv$.\n\nOnly three cells had to change, moving it from the in review section to the academic articles section, tweaking the title, and updating the url.\n\nIn RStudio, the CV is ready to go. All I had to do was click the Knit button and wait for the pdf.\nThe resume took ever so slightly more work. For space and aesthetic reasons I use a non-default formatting for my publications. This is a manual step but is not hard at all. All I did was:\n\nRead in the position data and run line 433 position_data %&gt;% print_section('academic_articles'). This produces markdown formatted text. Each line of text is treated as a separate “item” and will be formatted according to some rules.\nCopy the markdown formatted text for the new publication.\nPaste it into the document and tweak the formatting:\n\nUse a smaller font size for everything between &lt;font size=\"1\"&gt; and &lt;/font&gt; .\nBold my name using ***\nDisplay authors and link together as the second line and don’t apply the third line’s formatting rules to anything (N/A)\n\n\n\nWith those edits made I click the “Knit” button again et voilà! Resume updated and ready to go."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#bonus-updating-values-in-the-text",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#bonus-updating-values-in-the-text",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Bonus: Updating values in the text",
    "text": "Bonus: Updating values in the text\nOne of the coolest tricks you can do if your resume is in R is to update text dynamically. You don’t have to search through and count how many students you’ve mentored, or papers you published2.\nHere’s a simple example. I want to include how long I’ve been using R, but I don’t want to have to update that by hand. You could calculate this in R like so:\n\ntoday     &lt;- as.Date(format(Sys.time(), \"%Y-%m-%d\")) # Get today's date\nstart_R   &lt;- as.Date(\"2017-01-29\")                   # Set starting date\ndays_diff &lt;- difftime(today, start_R)                # Calc. days elapsed\nyears     &lt;- as.numeric(days_diff) / 365             # Convert to years\nyears     &lt;- round(years)                            # Round\nyears\n\nRMarkdown let’s you embed this calculation in the text. In my documents I have something like this “R Programming (7years )” which will show up in the pdf as “R Programming (# years)”."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#footnotes",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#footnotes",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the bonus at the very end.↩︎\nLook at line 430 in Resume.Rmd↩︎"
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html",
    "title": "Solving the Wrong Problem",
    "section": "",
    "text": "There’s a perspective that “it is better to know nothing than to know what ain’t so.”1 In my experience this is certainly the case with debugging because “knowing” will lead you down a rabbit trail of trying to solve the wrong problem.\nThe approach that works well for me is to “trust but verify” your knowledge. If your initial attempts to fix the bug in your code don’t work, take some time to check your assumptions – specifically your assumptions about where the bug is. This slows down your work initially because you’re often testing things that are behaving as you expect expectations, but this saves you from spending a lot of time trying to fix the wrong problems."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#what-you-know-that-aint-so",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#what-you-know-that-aint-so",
    "title": "Solving the Wrong Problem",
    "section": "",
    "text": "There’s a perspective that “it is better to know nothing than to know what ain’t so.”1 In my experience this is certainly the case with debugging because “knowing” will lead you down a rabbit trail of trying to solve the wrong problem.\nThe approach that works well for me is to “trust but verify” your knowledge. If your initial attempts to fix the bug in your code don’t work, take some time to check your assumptions – specifically your assumptions about where the bug is. This slows down your work initially because you’re often testing things that are behaving as you expect expectations, but this saves you from spending a lot of time trying to fix the wrong problems."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#a-recent-example",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#a-recent-example",
    "title": "Solving the Wrong Problem",
    "section": "A recent example:",
    "text": "A recent example:\nI’m writing a model that uses relationships between genes to predict a trait. The problem is that the model isn’t something I can write by hand (there are 6,067 inputs) and is way to slow – I’ve estimated it would take about 1.7 days to complete a single epoch (training cycle).\n\n\n\nHere are all the model’s processes for just two input genes.\n\n\nIn the diagram above, data from each gene (the nodes at the top) is fed into different functions (nodes 0-23) representing associations between different biological processes until they reach the output node (24) which predicts the trait.\nSome nodes share the same input (here node 14 and 10 both need node 11 as input). Under the hood I have the model storing the output of these nodes so it doesn’t have to re-calculate outputs un-necessarily (here the model would look-up the output of node 11 instead of recalculating it). This seems to work nicely but is a little unusual – in over two years this is the first time I’ve manually done this sort of thing.\nBecause of that, when I move my model from a tiny demo data set to the real thing and saw it was slow as molasses I “knew” my model slow because it was storing and retrieving intermediate results.\nOne assumption underlying this is that the model library is effectively designed and optimized such that it’s easier to get worse performance by doing unconventional things than better performance. This isn’t a bad assumption most of the time but we’ll see how it got me thinking about the wrong problem. My thought process went something like this:\n“Okay, so I’m doing something a little unconventional by looking up module outputs. Maybe if I can rewrite the model without this, some ☆Pytorch magic☆ will happen improving training speed.”\n“Hmm, the most straightforward way to write a model would be to chain the inputs and outputs like so”\ninput_1 = x_list[0]\nmodule_2 = x_list[1]\nintermediate_1 = module_1(input_1)\nintermediate_2 = module_2(input_2)\noutput  = module_3(nn.Concatenate([intermediate_1, intermediate_2], axis = 1))\n“But it would be unfeasible to do this because I’d have to write a line for each input and process 8,868 in total… or would it?”\nThis should have seemed like a totally unreasonable thing to do and been where I stopped to think if there was another way to get a speed increase (or tested this by writing a tiny neural net by hand with and without caching results and looked for a tiny difference in speed). However, years ago I met a class deadline by using python2 to write python2 code so this seemed perfectly feasible.\nSo the plan became :\n\nGenerate a boat load strings containing python code\nUse Python’s exec() and eval() functions to run each string\nSit back and think about what a clever idea it was having my code write my code.\n\nSeveral hours later I’ve learned a fair bit about how exec() and eval() handle scope and that their behavior between python2 and python3 has changed and still have no working model. So I decide to print the code I wanted executed to the console paste it (all 8,868 lines of it) into my model definition, and run it.\nThis solution was inelegant but quick to implement and exactly what needed to happen because the model didn’t perform any better. If anything it was slower, so there definitely wasn’t any ☆Pytorch magic☆ happening. This was a big enough surprise that it got me to question if the model was the problem after all instead of running down other rabbit trails."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#so-wheres-the-problem",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#so-wheres-the-problem",
    "title": "Solving the Wrong Problem",
    "section": "So where’s the problem?",
    "text": "So where’s the problem?\nBuilding a model may be the most evocative part of the data science workflow, but the steps that precede it are often as or more important. Choices around how to handle missing or potentially unrepresentative data are important as are how data is stored and moved around. In this case, I wasn’t thinking about these critical choices.\nFor each individual, there are data for genes throughout its genome (x_list, a list where each entry is a gene’s SNPs), and it’s trait of interest (y). Here’s the (simplified) code for this data set:\nclass ListDataset(Dataset):\n    def __init__(self, y, x_list):\n        self.y = y \n        self.x_list = x_list\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        # Get the data for index `idx`\n        y_idx =self.y[idx]\n        x_idx =[x[idx, ] for x in self.x_list]\n        \n        # Move to GPU so model can access it\n        y_idx.to('cuda')\n        x_idx = [x.to('cuda') for x in x_idx]\n        return y_idx, x_idx\nDo you spot what’s happening? When __getitem__ loads an observation, it has to move data from each gene to the GPU. This process isn’t instantaneous and is happening for each of the 6,067 genes every time an observation is loaded.\nTraining a network with a mere 100 observations (batch size of 10) takes 101.89s/it but if all the data is moved to the GPU before its 15% faster at 86.34s/it.\nThat’s nice, but since there are over 80,000 observations, it’s not enough to make training this model feasible. There’s another place we can look for improvements, and that’s the batch size. Increasing the batch size will mean that more observations are being moved to the GPU at a time so it has to happen fewer times. In this example getting all training observations in a single batch makes training 86% faster at 13.44s/it."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#take-home",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#take-home",
    "title": "Solving the Wrong Problem",
    "section": "Take home:",
    "text": "Take home:\nTesting your assumptions (especially while debugging) is like insurance. When you’re on the right track from the start, it’ll cost you a little time that you otherwise wouldn’t have spent but it’ll keep you from spending a lot of time trying to solve the wrong problem.\npost script:\nEven solving the right problem the result may not be what you want. Extrapolating from a more realistic subset of the data results in an estimated 5.6 hours per epoch. Better than 1.7 days, but not a home run ."
  },
  {
    "objectID": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html",
    "href": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html",
    "title": "Capturing Tacit Knowledge Through Blogging",
    "section": "",
    "text": "One of my goals with mentoring is to help others avoid places I got stuck (or get through them more easily) when I was learning. Part of this is writing guides, documentation, SOPs, etc. that I wish I had, but there’s some knowledge that doesn’t fit into a nice long form documents.\nTacit knowledge (e.g. how to approach different problems) or helpful little tricks aren’t well suited to that sort of format. Posts in this series are designed to collect thoughts in this vein to hopefully help out a fellow computational traveler starting their journey.\nTo that end these posts will be a mix of tricks, advice, retrospectives, and maybe a few guides that don’t fit in the protocols/ folder."
  },
  {
    "objectID": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html#advice-à-la-carte",
    "href": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html#advice-à-la-carte",
    "title": "Capturing Tacit Knowledge Through Blogging",
    "section": "",
    "text": "One of my goals with mentoring is to help others avoid places I got stuck (or get through them more easily) when I was learning. Part of this is writing guides, documentation, SOPs, etc. that I wish I had, but there’s some knowledge that doesn’t fit into a nice long form documents.\nTacit knowledge (e.g. how to approach different problems) or helpful little tricks aren’t well suited to that sort of format. Posts in this series are designed to collect thoughts in this vein to hopefully help out a fellow computational traveler starting their journey.\nTo that end these posts will be a mix of tricks, advice, retrospectives, and maybe a few guides that don’t fit in the protocols/ folder."
  },
  {
    "objectID": "posts/DanielKick/230524_linux_do_nothing_scripting/index.html",
    "href": "posts/DanielKick/230524_linux_do_nothing_scripting/index.html",
    "title": "Tips: Do-nothing Scripting in Bash",
    "section": "",
    "text": "Do-nothing scripting is a nice way to blend documenting a protocol with running it. You can use this template as a place to start:\n#!/usr/bin/bash\n#-----------------------------------------------------------------------------#\nSTEP='Step 0:'\necho \"$STEP\"\necho \"Run? (y/n)\"; read -n 1 k &lt;&1\nif [[ $k = n ]] ; then\nprintf \"\\nSkipping $STEP\\n\"; fi\nelse\nprintf \"\\nDoing $STEP\\n\"\n# Code for step here:\nNote, having the condition be on n instead of yes allows for the code (which will vary in length) to be at the end. This makes the control flow easy to see."
  },
  {
    "objectID": "posts/DanielKick/220928_python_trivia_missing/index.html",
    "href": "posts/DanielKick/220928_python_trivia_missing/index.html",
    "title": "Trivia: In Python Missing Isn’t Equal to Itself",
    "section": "",
    "text": "Python quirk I just learned and think is worth sharing. A missing valued doesn’t equal itself.\nHere’s the context: I’m making a list of values from a column that could not be converted to a date. Missing values can’t be converted so they end up in the list (e.g. [nan, '7/5/21 for pass 2']. So how do we discard this empty value? We use a list comprehension to see if the value is equal to itself ( [val for val in my_list if val == val] ) and will get a nan free list."
  },
  {
    "objectID": "posts/DanielKick/220323_python_caching_pickle/index.html",
    "href": "posts/DanielKick/220323_python_caching_pickle/index.html",
    "title": "Tips: Cache Intermediate Results with pickle",
    "section": "",
    "text": "Here’s a useful pattern I’ve been getting a lot of mileage out of lately. If you’re running an analysis that has a time consuming step you can save the result as a python readable “pickle” file. Addendum: In some cases pickling a python objects can sometimes succeed in storing and retrieving data where a library’s built in functions for saving/loading data fails.\nimport pickle as pkl\n\npath = \"./data_intermediates/processed_data.pkl\"\nif os.path.exists(path):\n    processed_data = pkl.load(open(path, 'rb'))\nelse:\n    # Make `processed_data` here\n    pkl.dump(processed_data, open(path, 'wb'))\nThis also lets you batch a process so that you can do more with your resources. For example here’s a list comprehension that will (for each day from 0-287) rearrange the weather data to be in “long” format. This is concise but requires processing the whole list at once which takes a lot of resources.\nsal_long_list = [_get_weather_long(results_list = res,\n                                   current_day = ith_day) for ith_day in np.linspace(start = 0, stop = 287, num = 288)]\nIf we incorporate it into the pattern above we can hold fewer items in memory at a time and then merge them (e.g. with list.extend() ) after the fact.\nfor ii in range(3):\n    file_path = '../data/result_intermediates/sal_df_W_long_part_day'+['0-95', \n                                                                       '96-191', \n                                                                       '192-287'][ii]+'.pkl'\n    if os.path.exists(file_path):\n        sal_long_list = pkl.load(open(file_path, 'rb'))\n\n    else:\n        # The original list comprehension is here, \n        # just made messier by selecting a subset of the indices.\n        sal_long_list = [_get_weather_long(                                \n            results_list = res,\n            current_day = current_day) for current_day in [\n            [int(e) for e in np.linspace(start = 0, stop = 95, num = 96)],   # Batch 1\n            [int(e) for e in np.linspace(start = 96, stop = 191, num = 96)], # Batch 2\n            [int(e) for e in np.linspace(start = 192, stop = 287, num = 96)] # Batch 3\n        ][ii]\n        ]\n        pkl.dump(sal_long_list, open(file_path, 'wb'))"
  },
  {
    "objectID": "posts/DanielKick/210713_python_custom_functions/index.html",
    "href": "posts/DanielKick/210713_python_custom_functions/index.html",
    "title": "Tips: Reusing Custom Functions",
    "section": "",
    "text": "Amendment: For packaging functions also see nbdev.\nI wanted to reuse a custom function across a few scripts without having copies of the same code in each script. The solution I found is to set up a module to hold these functions. This seems straightforward once you know how it’s done.\n\nSet up a directory containing your functions and a blank file called __init__.py.\n\nAdd the directory containing your module directory to the system path (here MaizeModel instead of MaizeModel\\\\library). If you’re on OSX or linux you’ll probably use single forward slashes instead of double backslashes.\n\nFinally import and call your functions.\n\nCaveats:\n\nIt seems that the system path isn’t permanently altered by sys.path.append, so one would need that at the start of the script or modify it some other way.\nIf your custom functions are in the in the same directory as your script, I think you can skip all of this and just import them.\nIf your functions are in a sub-directory of the same directory as your script, I think you can get away without adding the directory to the path."
  },
  {
    "objectID": "posts/DanielKick/210615_python_graph_gallery/index.html",
    "href": "posts/DanielKick/210615_python_graph_gallery/index.html",
    "title": "Tips: Find the Graph you want in using a Graph Gallery",
    "section": "",
    "text": "This or similar sites can be helpful for looking up the right code/library for a plot. You can also find library specific ones. (matplotlib, plotly)\nOne of R’s main plotting libraries, ggplot2, describes plots by layering one component on top of another (e.g. starting with x and y variables, adding points, adding error bars, adding aesthetic adjustments). If that sort of approach appeals to you there is a python version of this library called plotnine (github, example use)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Washburn Lab Resources",
    "section": "",
    "text": "Rover Use in Field\nRover Battery Charging\n\n\n\nStitching Images with Pix4Dmapper\nStitching Images with Pix4Dmapper: RE-mx Drone\nGridding the Reference Flight (QGIS)\n\n\n\n\n\n\nRestarting the Image Script on the Rootbot’s Pi\n\n\n\n\n\n\nTransferring Data Between Computers\n“Do Nothing” Scripting to Progressively Automate tasks\n\n\n\nSingularity: RStudio in a box\nUsing Port Forwarding to Access Your Jupyter Notebook\nMaking a Singularity Container From Your Conda Environment\nUsing your Singularity container with Open OnDemand\n In progress: \nOverview: Suggested HPC Workflow\nUsing Open OnDemand\n\n\n\nAdding Protocols\n In progress: \nWebsite Overview"
  },
  {
    "objectID": "index.html#field-work",
    "href": "index.html#field-work",
    "title": "Washburn Lab Resources",
    "section": "",
    "text": "Rover Use in Field\nRover Battery Charging\n\n\n\nStitching Images with Pix4Dmapper\nStitching Images with Pix4Dmapper: RE-mx Drone\nGridding the Reference Flight (QGIS)"
  },
  {
    "objectID": "index.html#lab-work",
    "href": "index.html#lab-work",
    "title": "Washburn Lab Resources",
    "section": "",
    "text": "Restarting the Image Script on the Rootbot’s Pi"
  },
  {
    "objectID": "index.html#computational-work",
    "href": "index.html#computational-work",
    "title": "Washburn Lab Resources",
    "section": "",
    "text": "Transferring Data Between Computers\n“Do Nothing” Scripting to Progressively Automate tasks\n\n\n\nSingularity: RStudio in a box\nUsing Port Forwarding to Access Your Jupyter Notebook\nMaking a Singularity Container From Your Conda Environment\nUsing your Singularity container with Open OnDemand\n In progress: \nOverview: Suggested HPC Workflow\nUsing Open OnDemand\n\n\n\nAdding Protocols\n In progress: \nWebsite Overview"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Lab",
    "section": "",
    "text": "Badges: [💻🧙, ]\nHarper LaFond joined the Washburn Lab in 2022 as the lab technician. She received her B.S. in Fisheries and Wildlife and her M.S. in Plant, Insect & Microbial Sciences specializing in entomology, both at the University of Missouri – Columbia. Harper has enjoyed working an array of field technician jobs with all sorts of living things from bugs and birds of prey to sturgeon and river carp. She then worked at the University of Missouri, Grape and Wine Institute as the viticulture extension specialist before joining the maize group at the USDA (she only works with crops that can be fermented into tasty drinks). Harper enjoys listening to podcasts on her walks to and from work, cooking dinner with her husband and sharing the leftovers with their dog, Chase.\n\n\n\n\n\n\n\n\n\nDaniel joined in 2021 and works with predicting phenotypes over a range of genotypes and environments. His public notes can be found here and personal page can be found here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBadges: [💻🧙, ]\nMadi can be found here."
  },
  {
    "objectID": "about.html#permanent-members",
    "href": "about.html#permanent-members",
    "title": "About the Lab",
    "section": "",
    "text": "Badges: [💻🧙, ]\nHarper LaFond joined the Washburn Lab in 2022 as the lab technician. She received her B.S. in Fisheries and Wildlife and her M.S. in Plant, Insect & Microbial Sciences specializing in entomology, both at the University of Missouri – Columbia. Harper has enjoyed working an array of field technician jobs with all sorts of living things from bugs and birds of prey to sturgeon and river carp. She then worked at the University of Missouri, Grape and Wine Institute as the viticulture extension specialist before joining the maize group at the USDA (she only works with crops that can be fermented into tasty drinks). Harper enjoys listening to podcasts on her walks to and from work, cooking dinner with her husband and sharing the leftovers with their dog, Chase."
  },
  {
    "objectID": "about.html#post-docs",
    "href": "about.html#post-docs",
    "title": "About the Lab",
    "section": "",
    "text": "Daniel joined in 2021 and works with predicting phenotypes over a range of genotypes and environments. His public notes can be found here and personal page can be found here."
  },
  {
    "objectID": "about.html#undergraduate-researchers",
    "href": "about.html#undergraduate-researchers",
    "title": "About the Lab",
    "section": "",
    "text": "Badges: [💻🧙, ]\nMadi can be found here."
  },
  {
    "objectID": "about.html#post-docs-1",
    "href": "about.html#post-docs-1",
    "title": "About the Lab",
    "section": "Post Docs",
    "text": "Post Docs\n\nPiyush Pandey (PhD)"
  },
  {
    "objectID": "about.html#undergraduate-researchers-1",
    "href": "about.html#undergraduate-researchers-1",
    "title": "About the Lab",
    "section": "Undergraduate Researchers",
    "text": "Undergraduate Researchers\n\nMia Ruppel\n\n\nBrady Blanton"
  },
  {
    "objectID": "about.html#staff",
    "href": "about.html#staff",
    "title": "About the Lab",
    "section": "Staff",
    "text": "Staff\n\nKate Guill"
  },
  {
    "objectID": "posts/DanielKick/210609_python_jupyter_plugins/index.html",
    "href": "posts/DanielKick/210609_python_jupyter_plugins/index.html",
    "title": "Tips: Jupyter Plugins",
    "section": "",
    "text": "I came across a handy set of tools for jupyter. There are of extensions for the notebooks that give you access to code snips, autocomplete by default, rendering a notebook as a slide show, and other features. To get it installed within an anaconda virtual environment you may only need to install it with this command:\nconda install -c conda-forge jupyter_contrib_nbextensions\nI not all of the extensions were showing up for me until I also ran these two lines, so it may take a bit of fiddling to get it to run.\njupyter contrib nbextension install --user\njupyter nbextension enable codefolding/main\nHere’s a linkto a page that shows some of these extensions in action."
  },
  {
    "objectID": "posts/DanielKick/210621_python_data_readability/index.html",
    "href": "posts/DanielKick/210621_python_data_readability/index.html",
    "title": "Tips: More Readable Data with pretty-print",
    "section": "",
    "text": "Here’s a tool that some may find useful when working with data that’s not yet in a [DataFrame]. It lets one “pretty-print” an object making any text that would wrap easier to read.\n# [In]\nprint(results_dictionary)\nprint(\"\\n --------------------------------------------- \\n\")\nimport pprint\npprint.PrettyPrinter(indent=4).pprint(results_dictionary)\n# [Out]\n{'active': True, 'additionalInfo:programDbId': '343', 'additionalInfo:programName': 'UC Davis', 'commonCropName': 'Cassava', 'contacts': None, 'culturalPractices': None, 'dataLinks': [], \n# ...\n'trialDbId': '343', 'trialName': 'UC Davis'}\n--------------------------------------------- \n{   'active': True,\n    'additionalInfo:programDbId': '343',    'additionalInfo:programName': 'UC Davis',    'commonCropName': 'Cassava',    'contacts': None,    'culturalPractices': None,    'dataLinks': [],    # ...    'trialDbId': '343',    'trialName': 'UC Davis'}"
  },
  {
    "objectID": "posts/DanielKick/220216_python_silent_replace/index.html",
    "href": "posts/DanielKick/220216_python_silent_replace/index.html",
    "title": "Tips: For those coming from R: Silent In Place Replacement",
    "section": "",
    "text": "Silent, in place assignment updating an object This tripped me up even though it’s consistent with how I’ve seen other objects behave. I needed an attribute to hold data extracted from a collection of files in a directory and created a class for this.\nclass hps_search_experiment:\n    def __init__(self, path=\"\", trial_type=''):\n        self.path = path\n        self.trial_type = trial_type\n        self.hps_best_trial = None\n        \n    def process_hps_files(self):\n        # ...\n        \n        self.hps_best_trial = hps_best_trial\nHowever, running like so fails.\ntest = hps_search_experiment(\n    path = './hps_search_intermediates_G/', \n    trial_type = 'rnr')\n    \ntest = test.process_hps_files()\ntest.hps_best_trial\n\n#&gt; AttributeError: 'NoneType' object has no attribute 'hps_best_trial'\nThis had me baffeld because I was thinking with R’s norms of data &lt;- data %&gt;% funciton() where in place replacement is the exception. Instead I needed to be thinking with python’s base object norms (e.g. a_list.extend(['b', 'c']) ). This fails because I overwrote test with the output of the method, which returns notthing since it’s overwriting attributes within test’s scope.\nThese would also work to update the attribute:\nself.hps_best_trial = hps_best_trial\n\nhps_search_experiment.__setattr__(self, \"hps_best_trial\", hps_best_trial)\n\n# if it's initialized as a list\nself.hps_best_trial.append([hps_best_trial]) \n# if a dict is initialized for data\nself.data = {'a':1}\nself.data.update({'hps_best_trial':hps_best_trial})"
  },
  {
    "objectID": "posts/DanielKick/220523_r_comments_in_tables/index.html",
    "href": "posts/DanielKick/220523_r_comments_in_tables/index.html",
    "title": "Trivia: R can have Comments in Tables",
    "section": "",
    "text": "R allows for comments to exist in tables. If there’s a # in the table you’re reading (e.g. as a part of a column name like chromosome#) then it can cause an unequal number of values between rows (everything on that line following it is ignored). The solution is to specify the comment character explicitly to be used (it can be ’’ to have no comment characters). Here’s an example:\necho \"a, b, c#, d\" &gt; test_table.txt\n&gt; Rscript -e \"read.table('test_table.txt')\"\n#   V1 V2 V3\n# 1 a, b,  c\n&gt; Rscript -e \"read.table('test_table.txt', comment.char = '')\" # with no comment character, all entries will be read\n#   V1 V2  V3 V4\n# 1 a, b, c#,  d"
  },
  {
    "objectID": "posts/DanielKick/221027_hpc_access_running_session/index.html",
    "href": "posts/DanielKick/221027_hpc_access_running_session/index.html",
    "title": "Tips: Open a new Interactive Session in Running Session",
    "section": "",
    "text": "A handy trick with batched processes on an HPC is that you can start an interactive session in a running session. Here’s an example where I needed to check if I was nearing the maximum allowed memory:\nHere I list my active jobs to get the jobid, run bash on that node, and list the processes by memory usage.\n[daniel.kick@Atlas-login-1 BLUP_G]$ squeue -u daniel.kick\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           2491772     atlas   BLUP-W daniel.k  R      39:56      1 Atlas-0025\n[daniel.kick@Atlas-login-1 BLUP_G]$ srun --pty --jobid 2491772 bash\n[daniel.kick@Atlas-0025 BLUP_G]$ htop"
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html",
    "title": "Simulation as a Super Power",
    "section": "",
    "text": "Writing simulations is one of the best ways I’m aware of to build one’s statistical intuition and comfort with data visualization. In addition to being able to try out new statistical tests and know exactly what effects they should find, they’re also great for communicating ideas and persuading others.\nA few months ago I had occasion to do just that.\n\\[...\\]\nAt the time I was advocating in a manuscript that when one needs to make a prediction combining predictions from different models is the way to go. Specifically, my results suggest that using a weighted average to make accurate models more influential. To do this, the predictions from each model are multiplied by the inverse of the model’s root mean squared error (rmse) of the model and summed. Someone helping me improve this manuscript thought that instead I should be weighting by the inverse of the model’s variance. This is a reasonable expectation (variance weighting is beautifully explained here) so I needed to convince my collaborator before the manuscript was ready for the world – Here’s how I did this with in a simulation that was only about 100 lines1 of R code."
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#simulating-observations",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#simulating-observations",
    "title": "Simulation as a Super Power",
    "section": "Simulating Observations",
    "text": "Simulating Observations\nLet’s imagine we’re studying dead simple system where the output is equal to the input (\\(y = 0 + 1*x\\)). We can simulate as many samples as we would like from this system over a range of xs.\n\nn = 100 # samples to be generated\nxs &lt;- seq(0, 20, length.out = n) # x values evenly spaced from 0-20 \nys &lt;- 0+1*xs\n\nHere’s the simulated “data”.\n\n\n\n\n\nNow we can simulate models that are trying to predict y. To do this we’ll think of a model as being equivalent to the true value of y plus some model specific error. If we assume that the models aren’t prone to systematically over or underestimating, then we can use a normal distribution to generate these errors like so:\n\nmean1 =  0 # error doesn't tend to be over or under\nvar1  =  1 # variance of the error\ny1 &lt;- ys + rnorm(n = n, mean = mean1, sd = sqrt(var1))\n\nWe can simulate a better model by decreasing the variance (errors are consistently closer to the mean of 0). Conversely we can simulate a worse model by making the model tend to over or undershoot by changing the mean or make larger errors more common by increasing the varience. Here’s a model that’s worse than the previous one.\n\nmean2 =  1 # predictions tend to overshoot\nvar2  = 10 # larger errors are more common\ny2 &lt;- ys + rnorm(n = n, mean = mean2, sd = sqrt(var2))\n\nLet’s look at the predictions from model y1 and model y.\n\n\n\n\n\nHere we can see that y1’s error (vertical lines) are considerably smaller than that of y2.\nWe can subtract the true value \\(y\\) from the predicted value \\(\\hat y\\) to see this more clearly.\n\n\n\n\n\nIn panel B we can see the difference between the two error distributions for the models (save a few irregularities in these distributions from only using 100 samples.\nNow we can try out different averaging schemes to cancel out some of the error and get a better prediction. We can test a simple average like so.\n\ne1 &lt;- 0.5*y1 + 0.5*y2\n\nWe can also try placing more weight on models with less variable predictions (and hopefully smaller errors).\n\nyhat_vars  &lt;- unlist(map(list(y1, y2), function(e){var(e)})) # Calculate variance for each model's predictions\nwght_vars  &lt;- (1/yhat_vars)/sum(1/yhat_vars) # Take the inverse and get percent weight by dividing by the sum \ne2 &lt;- wght_vars[1]*y1  + wght_vars[2]*y2 # Scale each model's prediction and add to get the weighted average.\n\nWe can also try placing more weigh on models that are more accurate2.\n\nyhat_rmses &lt;- unlist(map(list(y1, y2), function(e){sqrt((sum((ys-e)**2)/n))}))\nwght_rmses &lt;- (1/yhat_rmses)/sum(1/yhat_rmses)\ne3 &lt;- wght_rmses[1]*y1 + wght_rmses[2]*y2\n\nNow we can calculate the RMSE for both models and these weighed averages.\n\n\n\n\n\nname\ny_rmse\nmean1\nmean2\nvar1\nvar2\n\n\n\n\ny1\n1.023765\n0\n1\n1\n10\n\n\ny2\n3.218318\n0\n1\n1\n10\n\n\nunif\n1.796390\n0\n1\n1\n10\n\n\nvar\n1.670037\n0\n1\n1\n10\n\n\nrmse\n1.217204\n0\n1\n1\n10\n\n\n\n\n\n\n\ny1 is the best set of predictions, averaging did not benefit predicitons here. This is not too much of a shock since y2 was generated by a model that was prone to systematically overshooting the true value and was more likely to have bigger errors.\nBut how would these results change if the models were more similar? What if the models had more similar error variences? Or if one was prone to overshooting while the other was prone to undershooting?"
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#expanding-the-simulation",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#expanding-the-simulation",
    "title": "Simulation as a Super Power",
    "section": "Expanding the Simulation",
    "text": "Expanding the Simulation\nTo answer this we can package all the code above into a function with variables for each models’ error distribution and how many observations to simulate and return the RMSE for each model. This function (run_sim) is a little lengthy so I’ve collapsed it here:\n\n\nCode\nrun_sim &lt;- function(\n    n = 10000,\n    mean1 = 0,\n    mean2 = 1, \n    var1 = 1,\n    var2 = 10\n  ){\n  xs &lt;- seq(0, 20, length.out = n)\n  ys &lt;- 0+1*xs\n\n  # Simulate models\n  y1 &lt;- ys + rnorm(n = n, mean = mean1, sd = sqrt(var1))\n  y2 &lt;- ys + rnorm(n = n, mean = mean2, sd = sqrt(var2))\n  \n  # Equal weights\n  e1 &lt;- 0.5*y1 + 0.5*y2\n  \n  # Variance weights\n  yhat_vars  &lt;- unlist(map(list(y1, y2), function(e){var(e)}))\n  wght_vars  &lt;- (1/yhat_vars)/sum(1/yhat_vars)\n  e2 &lt;- wght_vars[1]*y1 + wght_vars[2]*y2\n  \n  # RMSE weights\n  yhat_rmses &lt;- unlist(map(list(y1, y2), function(e){sqrt((sum((ys-e)**2)/n))}))\n  wght_rmses &lt;- (1/yhat_rmses)/sum(1/yhat_rmses)\n  e3 &lt;- wght_rmses[1]*y1 + wght_rmses[2]*y2\n  \n  # Aggregate predictions and accuracy\n  data &lt;- data.frame(xs, ys, y1, y2, unif = e1, var = e2, rmse = e3)\n  plt_data &lt;- data %&gt;% \n    select(-xs) %&gt;% \n    pivot_longer(cols = c(y1, y2, unif, var, rmse)) %&gt;% \n    rename(y_pred = value) %&gt;% \n    # Calc RMSE\n    group_by(name) %&gt;%                \n    mutate(y_se = (ys - y_pred)**2) %&gt;% \n    summarise(y_rmse = sqrt(mean(y_se))) %&gt;% \n    ungroup() %&gt;% \n    mutate(mean1 = mean1,\n           mean2 = mean2,\n           var1 = var1,\n           var2 = var2)\n  \n  return(plt_data)\n}\n\n\nNext we’ll define the variables to examine in our computational experiment.\nWe can think about combining models that differ in accuracy (error mean) and precision (error variation). These differences can be are easier to think about visually. Here are the four “flavors” of model that we would like to combine to test all combinations of accuracy and precision.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nSpecifically, We’ll have one model that acting as a stable reference and vary the error of the other (y2). We’ll make a version of the y2 model that is accurate (the mean error is only shifted by 0.01) and one that is inaccurate (the mean error is shifted by 50). Then we’ll see what happens when these two go from being precise (variance around 0.01) to very imprecise (variance up to 100).\nIn R this is expressed as below. When the mean of the error (mean_shift) is near zero accuracy is high. When the variance of the error (var_shift) is near zero precision is high.\n\nparams &lt;- expand.grid(\n  mean_shift = seq(0.01, 50, length.out = 2),\n  var_shift = c(seq(0.01, 0.99, length.out = 50), seq(1, 100, length.out = 90))\n)\n\nThis results in quite a few (280) combinations. of parameters Let’s look at the first and last few:\n\n\n\n\n\n\nmean_shift\nvar_shift\n\n\n\n\n1\n0.01\n0.01000\n\n\n2\n50.00\n0.01000\n\n\n3\n0.01\n0.03000\n\n\n278\n50.00\n98.88764\n\n\n279\n0.01\n100.00000\n\n\n280\n50.00\n100.00000\n\n\n\n\n\n\n\nNow we’ll generate the results. This code may look confusing at first. Here’s what it’s doing. 1. run_sim executes the steps we did above, using the parameters we specified in params to generate 100,000 observations and calculate the expected RMSE of each approach. 1. map is a way of looping over some input and putting all of the results into a list. In this case we’re looping over all the the rows in params so we will end up with a list containing a data.frame for each of the 280 parameter combinations. 1. rbind will combine two data.frames, ‘stacking’ one on top of the other. However, it can’t use a list as input so… 1. we have to use do.call. It will iteratively apply a function (rbind) to the entries in a list so that all the simulation results end up in one big data.frame.\n\nsim_data &lt;- do.call(         # 4.\n  rbind,                     # 3\n  map(seq(1, nrow(params)),  # 2.\n      function(i){           \n        run_sim(             # 1.\n          n = 10000,\n          mean1 = 0,\n          mean2 = unlist(params[i, 'mean_shift']), \n          var1 = 1,\n          var2 = unlist(params[i, 'var_shift']))\n      })\n)\n\nOnce this runs we can look at the results. Let’s consider the high accuracy y2 first, starting where y2’s variance is less than or equal to y1’s variance (1).\n\n\n\n\n\nWhen y2’s variance is very small (&lt; ~0.2) it outperforms all other estimates (just like the previous simulation). As it increases it crosses the line for \\(rmse^{-1}\\) weighting (rmse, orange line) and then the other averaging schemes before converging with y1 (dashed blue line). Over the same span \\(rmse^{-1}\\) converges with \\(var^{-1}\\) (var, red line), and the simple average (unif, black line).\n\n\n\n\n\nAs y2 continues to worsen, every prediction (except those from y1) get worse and worse. What’s interesting is that this doesn’t happen at the same rate. Because \\(rmse^{-1}\\) weighting penalizes predictions from models based on accuracy its error grows much more slowly than \\(var^{-1}\\) weighting or uniform weighting.\nTo summarize – If two models are equally good (1 on the y axis) then using any of the averaging strategies here will be better than not averaging. If one is far and away better than the other then it’s best to ignore the worse one. In practice one might find they have models that are performing in the same ballpark of accuracy. These results would suggest that in that case one gets the best results by \\(rmse^{-1}\\) weighting.\nNow let’s add in the case where one model is highly inaccurate. In this case, as precision worsens y2 (top blue line) has higher error but this is hard to see given just how much error it has to begin with. Uniform weighting follows a similar trend (but lessened by half) while \\(var^{-1}\\) improves as y2 becomes more imprecise because this decreases it’s influence on the final prediction. Of the averages \\(rmse^{-1}\\) is the best by a country mile because it accounts for the inaccuracy of y2 right from the start."
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#what-if-models-err-in-different-directions",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#what-if-models-err-in-different-directions",
    "title": "Simulation as a Super Power",
    "section": "What if models err in different directions?",
    "text": "What if models err in different directions?\nJust for fun, let’s add one more simulation. Let’s suppose we have two models that are equally precise but err in opposite directions. We can modify the code above like so to have some combinations that are equally accurate (just in oppostie directions) and with differing accuracies.\n\nshift_array = seq(0.01, 10, length.out = 40)\nparams &lt;- expand.grid(\n  mean_shift  =    shift_array,\n  mean_shift2 = -1*shift_array\n)\n\nLet’s consider the case where errors are equal and opposite. In the previous simulation, when model variances were equal (1) the performance of all the averages converged, so we might expect that to be the case here. We can see the models getting worse and worse, but can’t see what’s happening with the averages.\n\n\n\n\n\nIf we zoom in, it looks like our intuition is correct (ignoring some sampling noise).\n\n\n\n\n\nBut we also simulated combinations where one model was off by more than the other. Let’s plot all the combinations of mean1 and mean2 but instead of showing the error of each method like we’ve done above, let’s instead just show where each method produces the best results.\n\n\n\n\n\nConsistent with what we’ve seen, for most of these combinations \\(rmse^{-1}\\) performs best. We can get a little fancier by color coding each fo these cells by the best expected error (y_rmse) and color coding the border with the method that produced the best expected error (excepting \\(rmse^{-1}\\) since that accounts for so much of the space).\n\n\n\n\n\nIt looks like there’s a sort of saddle shape off the diagonal. We’ll re-plot these data in 3d so we can usethe z axis for y_RMSE and color code each point as above.\n\n\n\n\n\n\nThere we go. Just a little bit of scripting and plotting will let one answer a whole lot of questions about statistics."
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#footnotes",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#footnotes",
    "title": "Simulation as a Super Power",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI added a fair bit more for the sake of this post↩︎\nFor simplicity we’re not using testing and training sets. In this simulation that shouldn’t be an issue, but one might consider cases where this could matter. For instance if one model was wildly over fit then its RMSE would not be predictive of its RMSE on newly collected data↩︎"
  },
  {
    "objectID": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html",
    "href": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html",
    "title": "Save only what you need",
    "section": "",
    "text": "[1] 7424.908\n\n\n[1] 735.05"
  },
  {
    "objectID": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#how-we-got-to-this-point",
    "href": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#how-we-got-to-this-point",
    "title": "Save only what you need",
    "section": "How we got to this point:",
    "text": "How we got to this point:\nCollecting data from many sites is expensive but using a biophysical model, many sites and years can be ‘harvested’ in minutes. I’m building a dataset with many cultivars planted across the united states. The problem is that I’m being greedy – I want to have the a day by day account of plant’s growth at ~1,300 locations from 1984-2022, varying cultivar, and planting date.\nIn my initial implementation the results from the model are written to a csv for each location…\n\n\n\nOh no.\n\n\nThis file has a boat load of data. It’s a table of 25,243,175 rows by 19 columns – 479,620,325 billion cells. By the end of the experiment much of my hard drive will be taken up by these."
  },
  {
    "objectID": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#reclaiming-88-storage-space",
    "href": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#reclaiming-88-storage-space",
    "title": "Save only what you need",
    "section": "Reclaiming 88% Storage Space",
    "text": "Reclaiming 88% Storage Space\nAn easy place to cut cells is from redundant or unneeded columns. These are produced by the simulation but the way I have the experiment structured, they aren’t needed after it’s done running.\n# YAGNI isn't just for code\ndf &lt;- df[, c(\n      # Indepenent Variables\n      'ith_lon', 'ith_lat', 'soils_i', 'SowDate', 'Genotype', 'Date',\n      \n      # Dependent Variables\n      'Maize.AboveGround.Wt', 'Maize.LAI', 'yield_Kgha'\n\n      # Redundant or not needed\n      #'X', 'ith_year_start', 'ith_year_end', 'factorial_file', 'CheckpointID', \n      #'SimulationID', 'Experiment', 'FolderName', 'Zone', 'Clock.Today'\n      )]\nThis is an easy way to get rid of over half the cells (down to 47.36%) (and really I should not have saved these in the first place) but we can do better still.\nMany of the rows represent times before planting without any data collected. All rows where Maize.AboveGround.Wt, Maize.LAI, and Maize.AboveGround.Wt are 0 can be dropped. Because so much of the year is out of the growing season this is quite helpful and cuts about half of the observations (20.09%).\nSplitting these data into two tables with independent variables or dependent variables (with a key) gets the total down to 10,602 + 53,530,975 = 53,541,577. Still a lot but only 11.16% of the starting size!\n\n\n\n\n\nData\nSize\nPercent Original\n\n\n\n\nOriginal\n479620325\n100.00\n\n\nSelect Cols.\n227188575\n47.37\n\n\nFilter Rows\n96355755\n20.09\n\n\nSplit Tables\n53541577\n11.16\n\n\n\n\n\n\n\nI could probably go even further, but now that each experiment takes up only 482 MB instead of 4.64 GB. Furhter optimization can wait for another day.\nWhile storage space is important (at this scale), another factor for the performance (and quality of life) is reading in the data. Using the basic read.csv function it takes 4 minutes 23 seconds to read in. Using the vroom library instead can read in these data in only 4.04 seconds."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html",
    "href": "posts/DanielKick/230915_vnn_overview/index.html",
    "title": "Making a “Visible” Neural Network",
    "section": "",
    "text": "In most neural networks, neurons are not parametric in the same way that linear models are. In a image recognition model there may be neuron which functions to detects edges but when the model is set up initially one can’t point to a neuron and say what it will do or represent. This can make interpreting the weights in a model tricky.\nVisible neural networks (VNN) are one way to get around this problem by making the structure of a model reflect the process being modeled. In a VNN, influential sub-components may be interpreted as implicating the process they represent as being important. Within biology VNNs have been used by Ma et al. 2018 and Hilten et al. 2021 working in yeast and humans respectively (in the later mixed performance, seemingly based on trait complexity)."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#whats-a-visual-neural-network",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#whats-a-visual-neural-network",
    "title": "Making a “Visible” Neural Network",
    "section": "",
    "text": "In most neural networks, neurons are not parametric in the same way that linear models are. In a image recognition model there may be neuron which functions to detects edges but when the model is set up initially one can’t point to a neuron and say what it will do or represent. This can make interpreting the weights in a model tricky.\nVisible neural networks (VNN) are one way to get around this problem by making the structure of a model reflect the process being modeled. In a VNN, influential sub-components may be interpreted as implicating the process they represent as being important. Within biology VNNs have been used by Ma et al. 2018 and Hilten et al. 2021 working in yeast and humans respectively (in the later mixed performance, seemingly based on trait complexity)."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#a-hypothetical-gene-network",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#a-hypothetical-gene-network",
    "title": "Making a “Visible” Neural Network",
    "section": "A hypothetical gene network",
    "text": "A hypothetical gene network\nBefore scaling to representing gene networks, I built a simple test case and will walk through it below, with all the necessary code (but some of it hidden1 for brevity).\n\nHere we have a hypothetical network which involves two genes (a1_input, a2_input), variants of which affect some initial processes (b1, b2), which in turn affect a second set of processes (c1, c2). I’ll use these last processes to predict my trait of interest (y_hat).\nThis is a directed acyclic graph, meaning that processes have an order (the arrows) and there are no loops (c1 doesn’t some how change a1_input). The model I’d like to end up with is a neural network with a structure that mirrors this graph 2 with each node representing one or more layers of neurons.\nBeginning with the end in mind, I need a way to specify: 1. The data the graph operates on 1. The process graph and each node’s attributes 1. How to “move” through the graph"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#the-data-itself",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#the-data-itself",
    "title": "Making a “Visible” Neural Network",
    "section": "1. The data itself",
    "text": "1. The data itself\nMy example trait, \\(y\\) is either 0 or 1 (plus a little noise). It’s controlled by two genes which are represented as tensor3 containing values for each possible nucleotide (ACGT) for each SNP measured in the gene. Conveniently, both genes either contain all 0’s or all 1’s and when there are only 0’s \\(y\\) will be around 0 (and the same for 1).\nThis of course means that in this population no nucleotides (all 0s) were observed or all nucleotides (all 1s) were simultaneously observed. Don’t ask me how this is possible 🤔. For real data these values would be probability of seeing a given nucelotide so “A” might be [1, 0, 0, 0]4.\n\nn_obs = 100 # 100 obs for each group\ny_true = torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, )),\n        np.ones( (n_obs, ))], 0)) + .1* torch.rand(2*n_obs,)\n        \ninput_tensor_dict = {\n    'a1_input': torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, 4, 3)),\n        np.ones( (n_obs, 4, 3))], 0)),\n    'a2_input': torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, 4, 2)),  \n        np.ones( (n_obs, 4, 2))], 0))}\n\nx_list_temp = [input_tensor_dict[key].to(torch.float) for key in input_tensor_dict.keys()]\nx_list_temp\n# output\n                        # Probability of\n[tensor([[[0., 0., 0.], # A\n          [0., 0., 0.], # C\n          [0., 0., 0.], # G\n          [0., 0., 0.]],# T\n\n         ...,\n\n         [[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]),\n\n tensor([[[0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.]],\n\n         ...,\n\n         [[1., 1.],\n          [1., 1.],\n          [1., 1.],\n          [1., 1.]]])]\n\nThen this data can be packaged nicely in a DataLoader5. This will retrieve the trait (y) and SNPs for each gene (in x_list) for 20 observations at a time.\n\ntraining_dataloader = DataLoader(\n  ListDataset(\n    y = y_true[:, None].to(torch.float32), # Set as 32 bit float to match network\n    x_list = [e.to(torch.float32) for e in x_list_temp]),\n    batch_size = 20,\n    shuffle = True)"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#defining-the-graph",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#defining-the-graph",
    "title": "Making a “Visible” Neural Network",
    "section": "2. Defining the graph",
    "text": "2. Defining the graph\nThe structure of a graph can be nicely represented as a python dictionary so I’ll begin with that:\n\nnode_connections = {\n  'y_hat':['c1', 'c2'],\n  'c1':['b1'],\n  'c2':['b2'],\n  'b1':['a1_input', 'b2'],\n  'b2':['a2_input'],\n  'a1_input': [],\n  'a2_input': []\n}\n\nEach node will have an input and output size stored in a dictionary. The output sizes are easy, all nodes will have the same size except for the last node, which has predicts y, which will have a size of 1.\n\nnode_list = list(node_connections.keys())\n\ndefault_output_size = 20\noutput_size_dict = dict(zip(node_list, \n                        [default_output_size for i in range(len(node_list))]))\noutput_size_dict['y_hat'] = 1 \noutput_size_dict\n# output\n{'a1_input': 20,\n 'a2_input': 20,\n 'b1': 20,\n 'b2': 20,\n 'c1': 20,\n 'c2': 20,\n 'y_hat': 1}\n\nThe input sizes are a little trickier. A node’s input should be the number of SNPs in a gene (if it’s an input node) or the sum of the outputs of the nodes on which it depends (e.g. y_hat’s input size is the sum of c1 and c2’s outputs). To do this, I’m going to copy the dictionary with all the connections between nodes, then swap the node names for their output sizes. Summing the list of these output values will be the required input size. Data nodes don’t depend on input from other nodes, so those will have an input shape of 0.\n\ninput_size_dict = node_connections.copy()\n\nno_dependants = [e for e in node_connections.keys() if node_connections[e] == []]\n\n# use the expected output sizes from `output_size_dict` to fill in the non-data sizes\ntensor_ndim = len(input_tensor_dict[list(input_tensor_dict.keys())[0]].shape)\nfor e in tqdm(input_size_dict.keys()):\n    # overwrite named connections with the output size of those connections\n    # if the entry is in no_dependants it's data so it's size needs to be grabbed from the input_tensor_dict\n    input_size_dict[e] = [\n        (list(input_tensor_dict[ee].shape)[1]*list(input_tensor_dict[ee].shape)[2]) \n        if ee in no_dependants\n        else output_size_dict[ee] for ee in input_size_dict[e]]\n\n# Now walk over entries and overwrite with the sum of the inputs\nfor e in tqdm(input_size_dict.keys()):\n    input_size_dict[e] = np.sum(input_size_dict[e])\n    \ninput_size_dict\n# output\n{'y_hat': 40,\n 'c1': 20,\n 'c2': 20,\n 'b1': 32,\n 'b2': 8,\n 'a1_input': 0.0,\n 'a2_input': 0.0}\n\nNow we can update the graph from above adding in the input/output sizes."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#how-to-move-through-the-graph",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#how-to-move-through-the-graph",
    "title": "Making a “Visible” Neural Network",
    "section": "3. How to move through the graph",
    "text": "3. How to move through the graph\nTo calculate the prediction for an observation each node in the graph needs to be run after all it’s input nodes have been run. Specifically, I need a list of nodes, ordered such that each node comes after all the nodes on which it depends.\nThis takes little doing. Here I use some custom helper function to find the unique entries in a dictionary, the “top” nodes (those on which no other nodes depend).\n\n# start by finding the top level -- all those keys which are themselves not values\n# helper function to get all keys and all value from a dict. Useful for when keys don't have unique values.\ndef find_uniq_keys_values(input_dict):\n    all_keys = list(input_dict.keys())\n    all_values = []\n    for e in all_keys:\n        all_values.extend(input_dict[e])\n    all_values = list(set(all_values))\n\n    return({'all_keys': all_keys,\n           'all_values': all_values})\n\n# find the dependencies for run order from many dependencies to none\n# wrapper function to find the nodes that aren't any other nodes dependencies.\ndef find_top_nodes(all_key_value_dict):\n    return([e for e in all_key_value_dict['all_keys'] if e not in all_key_value_dict['all_values']])\n\nSimilar to how I calculated each node’s output size, here I copy the connection dictionary and then manipulate it. I repeatedly identify the top-most nodes in the graph, add them to a list, and then remove them from the dictionary. Repeating this “peels” of the top layer over and over until there are nodes left. The resulting list is ordered from top most to most basal, so reversing it is all that need be done to get the order nodes should be run in.\n\n# find the dependencies for run order from many dependencies to none\ntemp = node_connections.copy()\n\ndependancy_order = []\n# Then iterate\nfor ith in range(100): \n    top_nodes = find_top_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n    if top_nodes == []:\n        break\n    else:\n        dependancy_order += top_nodes    \n        # remove nodes from the graph that are at the 'top' level and haven't already been removed\n        for key in [e for e in dependancy_order if e in temp.keys()]:\n             temp.pop(key)\n\n                \n# reverse to get the order that the nodes should be called\ndependancy_order.reverse()                \ndependancy_order\n# output\n['a2_input', 'a1_input', 'b2', 'b1', 'c2', 'c1', 'y_hat']"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#turn-the-graph-into-a-neural-network",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#turn-the-graph-into-a-neural-network",
    "title": "Making a “Visible” Neural Network",
    "section": "4. Turn the graph into a neural network",
    "text": "4. Turn the graph into a neural network\nSo far, we have the data in a useful format (training_dataloader), a description of what the network should look like (node_connections, input_size_dict, output_size_dict), and the order that nodes in the network should be run in (dependancy_order). With this, we can build the network. I’ll start by defining a node as a linear layer (nn.Linear) that is passed into a ReLU. By creating a function6 for making nodes, changing every node in the network is as easy as editing this function.\n\ndef Linear_block(in_size, out_size, drop_pr):\n    block = nn.Sequential(\n        nn.Linear(in_size, out_size),\n        nn.ReLU())\n    return(block)  \n\nNow, I can go through each node in order of it’s dependencies and have it return the data (if it’s an input node), process inputs with a Linear_block (if it’s not an input node or the output node), or use a linear function to predict the trait7.\n\n# fill in the list in dependency order. \nlayer_list = []\nfor key in dependancy_order:\n    if key in input_tensor_names:\n        layer_list += [\n            nn.Flatten()\n        ]\n    elif key != 'y_hat':\n        layer_list += [\n            Linear_block(in_size=example_dict_input_size[key], \n                         out_size=example_dict_output_size[key])\n                      ]\n    else:\n        layer_list += [\n            nn.Linear(example_dict_input_size[key], \n                      example_dict_output_size[key])\n                      ]"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#double-checking-the-model-structure",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#double-checking-the-model-structure",
    "title": "Making a “Visible” Neural Network",
    "section": "Double checking the model structure",
    "text": "Double checking the model structure\nUsing the lovely library torchviz, we can visualize every computational step in this model.\n\nThis is a lot to look at, but if we compare it to the earlier graph we can spot the same loop."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#the-moment-of-truth",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#the-moment-of-truth",
    "title": "Making a “Visible” Neural Network",
    "section": "The moment of truth…",
    "text": "The moment of truth…\nNow all that is left is to see if the model trains. Using the objects describing the graph, the names of the input tensors, and the order nodes should be run it I’ll initialze the network, train it for 200 epochs aaaaaannnnddd….\n\nmodel = NeuralNetwork(example_dict = node_connections, \n                      example_dict_input_size = input_size_dict,\n                      example_dict_output_size = output_size_dict,\n                      input_tensor_names = list(input_tensor_dict.keys()),\n                      dependancy_order = dependancy_order) \n\n\nmodel, loss_df = train_nn_yx(\n    training_dataloader,\n    training_dataloader, # For demo, the training and testing data are the same.\n    model,\n    learning_rate = 1e-3,\n    batch_size = 20,\n    epochs = 200\n)\n\nIt works!\n\nNow all that’s left is to scale it up to a full genome and all the connections between the genes in it 😅."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#footnotes",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#footnotes",
    "title": "Making a “Visible” Neural Network",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHave a look at the page source.↩︎\nThis will not be a graph neural network, although they may be effective here.↩︎\nA box of numbers that can have multiple dimensions. A matrix is a “rank-2” tensor.↩︎\nTechnically, with 4 possibilities you only need 3 binary digits where [0, 0, 0] would be 100% probability of the fourth nucleotide↩︎\nI’m using a custom Dataset subclass. See source for details.↩︎\nTechnically a method since it’s in a class.↩︎\nAs an aside, the first time I wrote this I had all non-input nodes be Linear_blocks. This resulted in fair bit frusterated debugging as the network would either train perfectly or fail to train depending on how the last ReLU was initialized🤦🏼‍♂️.↩︎"
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html",
    "title": "Worse is better and not doing things “right”",
    "section": "",
    "text": "“Worse is better” is an idea I get a lot of mileage out of. Here’s the crux of it:\nI find this is useful descriptively1 but also prescriptively as a way to spend less time doing work that doesn’t need to be done.\nIn brief the idea is that once you have something that works it’s often not worth altering it to make it faster, more efficient, or more elegant … at least initially. Optimization is important (example but what I’m talking about here premature optimization. Avoiding the urge to improve things that aren’t the priority can be difficult, especially when you conceptually know what you would change."
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#simplified-example",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#simplified-example",
    "title": "Worse is better and not doing things “right”",
    "section": "Simplified Example",
    "text": "Simplified Example\nHere’s an example: I’m building a network that ‘compresses’ information. The key idea is that there’s a function, f(), takes in some number of values and outputs fewer values. We can use this function over and over again to compress the values more and more. Once they’re ‘compressed’ we can do the reverse procedure and get more values until we’re back at the starting number.\nThere’s a catch however, and that’s that the function can only output an integer number of values even if the number should be a fraction. It’s like this division function. If the numerator argument is the number of input values and it will return numerator/3 values.\n\ndiv_it &lt;- function(numerator, divisor = 3){\n  res = numerator/divisor\n  res = round(res)\n  return(res)\n}\n\ndiv_it(100)\n\n[1] 33\n\n\nBecause it can only return whole numbers, we can’t reverse this procedure and always get back the same number – sometimes we have to add or subtract a little bit.\n\ninv_div_it &lt;- function(numerator, divisor = 3){\n  return(numerator*divisor)\n}\n\ninv_div_it(33)\n\n[1] 99\n\ninv_div_it(33)+1\n\n[1] 100\n\n\nIf we want to really compress the input (f(X) |&gt; f(X) |&gt; f(X) |&gt; f(X) or f(f(f(f(X))))) then the number of values at each level would be:\n\nvals &lt;- c(100)\nfor(i in 1:4){\n  i_val &lt;- vals[length(vals)]\n  vals[length(vals)+1] &lt;- div_it(i_val) \n}\nvals\n\n[1] 100  33  11   4   1\n\n\nIdeally running the inverse procedure multiple times on the last output above (just one value) would output produce:\n\nvals_reverse &lt;- vals[length(vals):1]\nvals_reverse\n\n[1]   1   4  11  33 100\n\n\nBut using the inverse function defined above (inv_div_it()) we get:\n\nrecover_vals &lt;- c(1)\nfor(i in 1:4){\n  i_val &lt;- recover_vals[length(recover_vals)]\n  recover_vals[length(recover_vals)+1] &lt;- inv_div_it(i_val) \n}\nrecover_vals\n\n[1]  1  3  9 27 81\n\n\nTo get back to 100 values we need to add a new value (imagine appending a 1 to an array) sometimes, and drop a value others, or make no change to output other times.\n\nadd_vals &lt;- c(1, -1, 0, 1)\n\nrecover_vals &lt;- c(1)\nfor(i in 1:4){\n  i_val &lt;- recover_vals[length(recover_vals)]\n  print(add_vals[i])\n  recover_vals[length(recover_vals)+1] &lt;- inv_div_it(i_val) + add_vals[i]\n}\n\n[1] 1\n[1] -1\n[1] 0\n[1] 1\n\nrecover_vals\n\n[1]   1   4  11  33 100\n\n\nWe could keep track of the remainder each time f() is called and use that to figure out when to add or subtract 1. That would be the elegant and efficient solution. We know the desired output (100 values) and the number of times f() was called (4) so we could also try changing the numbers in add_vals until we have four numbers that. This solution would be inelegant but still effective.\nIf a piece of code only needs to be a few times then the cost of the time you’d spend optimizing it will probably be worth more than than cost of the time the computer spends running it (see also).\nIf the sloppy way to express what you want is good enough then don’t worry about it. Good enough now is often better than perfect later."
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#example-in-context-python",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#example-in-context-python",
    "title": "Worse is better and not doing things “right”",
    "section": "Example in Context (python)",
    "text": "Example in Context (python)\nThe motivating problem behind this write up is that ‘compressing’ weather data (17 measurements for 365 days) into fewer values. I’m using a variation autoencoder with convolution layers which you can imagine as passing a sliding window over a 17x365 grid and summarizing each windowed chunk to get fewer values.\nTo check if the compression is effective, we have to compress 17x365 values down to something smaller (e.g. 17x23), and inflate them back to 17x365 so we can compare the input weather to the output weather. If we can get back the same 17x365 values (or something pretty close) then the comprssion is effective..\nFrom the input data’s length (days) you can calculate what a convolutional layer’s output length will be like so:\ndef L_out_conv1d(\n    L_in = 365, \n    kernel_size=3, stride = 2, padding=1, dilation = 1\n): return ((L_in +2*padding-dilation*(kernel_size-1)-1)/stride)+1\n\nL_out_conv1d(L_in = 365) # 183.0\nAnd the same for reversing the operation (with a transposed convolution).\ndef L_out_convT1d(\n    L_in = 183, \n    kernel_size=3, stride = 2, padding=1, output_padding=0, dilation = 1\n): return (L_in - 1)*stride-2*padding+dilation*(kernel_size-1)+output_padding+1\n\nL_out_convT1d(L_in = 183) # 365.0\nThe trouble is that if I stack convolution layers the output length can become a fraction, which is forced to an integer, and prevents the reverse operation from producing the right number. When I use 4 layers the length should be [365, 183.0, 92.0, 46.5, 23.75] which as integers is [365, 183, 92, 46, 23]. Reversing the operation produces [23, 45, 89, 177, 353].\nWe can get back to 365 days by increasing the output’s length in some of the transposed convolution layers by adding a non-zero output_padding. I don’t know how many layers will be best, so I can’t hard code these values. I could use the functions above to calculate what when the output_padding should be 0 and when it shouldn’t (the elegant solution), but that’s not what I did.\nInstead I made a simple disposable neural network just to check if I had the output_paddings right by tracking the lengths of the tensor after each layer.\n# input data. One observation, 17 measurements, 365 days of measurements. \n# It's all 0s because all I care about right now is the dimensions of the data.\nxin = torch.zeros((1, 17, 365))\n\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\n# encoder network\nne = nn.ModuleList([\n    nn.Sequential(nn.Conv1d(\n    17, out_channels=17, kernel_size= 3, stride= 2, padding  = 1), nn.BatchNorm1d(\n    17), nn.LeakyReLU())\n    for i in range(len(layer_output_padding))\n])\n\n# Decoder network\nnd = nn.ModuleList([\n    nn.Sequential(nn.ConvTranspose1d(\n    17, 17, \n    kernel_size=3, stride = 2, padding=1, output_padding=layer_output_padding[i]), nn.BatchNorm1d(\n    17), nn.LeakyReLU())\n    for i in range(len(layer_output_padding))\n])\nThen I can run this network …\n# list to store lengths\ntensor_Ls = []\n\n# add the input data's length (days)\ntensor_Ls += [list(xin.shape)[-1]] \n\n# encode data\nfor mod in ne:\n    xin = mod(xin)\n    tensor_Ls += [list(xin.shape)[-1]]\n\n# add the encoded data's \ntensor_Ls += [str(tensor_Ls[-1])]\n\n# decode data\nfor mod in nd:\n    xin = mod(xin)\n    tensor_Ls += [list(xin.shape)[-1]]\n… and look at the first and last value of the list of lengths (tensor_Ls) to see if the proposed output paddings will work.\ntensor_Ls[0] == tensor_Ls[-1]\n# False\ntensor_Ls\n# [365, 183, 92, 46, 23, 12, 6, 3, 2, '2', 3, 5, 9, 17, 33, 65, 129, 257]\nNext I need a way to systematically produce different paddings. For a decoder of four layers I would test paddings [0, 0, 0, 0], [1, 0, 0, 0], ... [1, 1, 1, 1] stopping at the first list that works. So I’ll write a function to increment [0, 0, 0, 0] to [1, 0, 0, 0].\ndef increment_list(\n    in_list = [0, 0, 0, 0],\n    min_value = 0,\n    max_value = 1):\n    # Check that all entries are within min/max\n    if False in [True if e &lt;= max_value else False for e in in_list]:\n        print('Value(s) above maximum!')\n    elif False in [True if e &gt;= min_value else False for e in in_list]:\n        print('Value(s) below minimum!')\n    elif [e for e in in_list if e != max_value] == []:\n        print('List at maximum value!')\n    else:    \n        # start cursor at first non-max value\n        for i in range(len(in_list)):\n            if in_list[i] &lt; max_value:\n                in_list[i] += 1\n                break\n            else:\n                in_list[i] = min_value\n    return(in_list)\n\nincrement_list()\n# [1, 0, 0, 0]\nThen we can loop through possible paddings until we find one that works or have tried all of them.\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\nwhile True:\n    # save a backup of the current padding\n    old_layer_output_padding = layer_output_padding.copy()\n    \n    \n    # ... define and run network here ...\n    \n    \n    # If it did work we're done\n    if True == (tensor_Ls[0] == tensor_Ls[-1]):\n        print('done!')\n        \n    # If the padding _didn't_ work change it\n    else:\n        layer_output_padding = increment_list(\n            in_list = layer_output_padding,\n            min_value = 0,\n            max_value = 1)\n    \n    # If the proposed new padding is the same as the backup, then we have tried all the possible paddings and will stop. \n    if layer_output_padding == old_layer_output_padding: \n        break\n\nAll together\nHere’s the full loop and its output:\n\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\nwhile True:\n    # save a backup of the current padding\n    old_layer_output_padding = layer_output_padding.copy()\n    \n    # input data. One observation, 17 measurements, 365 days of measurements. \n    # It's all 0s because all I care about right now is the dimensions of the data.\n    xin = torch.zeros((1, 17, 365))\n\n    # encoder network\n    ne = nn.ModuleList([\n        nn.Sequential(nn.Conv1d(\n        17, out_channels=17, kernel_size= 3, stride= 2, padding  = 1), nn.BatchNorm1d(\n        17), nn.LeakyReLU())\n        for i in range(len(layer_output_padding))\n    ])\n\n    # Decoder network\n    nd = nn.ModuleList([\n        nn.Sequential(nn.ConvTranspose1d(\n        17, 17, \n        kernel_size=3, stride = 2, padding=1, output_padding=layer_output_padding[i]), nn.BatchNorm1d(\n        17), nn.LeakyReLU())\n        for i in range(len(layer_output_padding))\n    ])\n    \n    # list to store lengths\n    tensor_Ls = []\n\n    # add the input data's length (days)\n    tensor_Ls += [list(xin.shape)[-1]] \n\n    # encode data\n    for mod in ne:\n        xin = mod(xin)\n        tensor_Ls += [list(xin.shape)[-1]]\n\n    # add the encoded data's \n    tensor_Ls += [str(tensor_Ls[-1])]\n\n    # decode data\n    for mod in nd:\n        xin = mod(xin)\n        tensor_Ls += [list(xin.shape)[-1]]    \n    \n    # If it did work we're done\n    if True == (tensor_Ls[0] == tensor_Ls[-1]):\n        print('done!')\n        \n    # If the padding _didn't_ work change it\n    else:\n        layer_output_padding = increment_list(\n            in_list = layer_output_padding,\n            min_value = 0,\n            max_value = 1)\n    \n    # If the proposed new padding is the same as the backup, then we have tried all the possible paddings and will stop. \n    if layer_output_padding == old_layer_output_padding: \n        break\n\n# done!\n\nlayer_output_padding  \n# [0, 1, 1, 0, 1, 1, 0, 0]\n\ntensor_Ls\n# [365, 183, 92, 46, 23, 12, 6, 3, 2, '2', 3, 6, 12, 23, 46, 92, 183, 365]\nThis may not be as not as elegant or as efficient as it could be, but it doesn’t matter. It only takes about 200ms so it’s not worth improving unless."
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#footnotes",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#footnotes",
    "title": "Worse is better and not doing things “right”",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ne.g. If scientific manuscripts with embedded code are valuable for reproducibility, why haven’t they become the default? There’s a lot of energy needed to switch and all of your collaborators already know word. 🤷🏼‍♂ ↩︎"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Writings from the Lab",
    "section": "",
    "text": "Daniel Kick"
  },
  {
    "objectID": "protocols/Drones/Pix4Dmapper_Stitch_RE-mx/index.html",
    "href": "protocols/Drones/Pix4Dmapper_Stitch_RE-mx/index.html",
    "title": "Stitching with Pix4Dmapper",
    "section": "",
    "text": "How to Use Pix4Dmapper to Stitch Drone Images (Micasense Camera – 10 Band (RE-mx))\n\nPix4Dmapper is installed on the lambda2 in the dry lab (NOT the Linux lambda)\nCheck what flights need to be stitched on the To Be Stitched List_2022: Teams &gt; UAV Missions &gt; Files\nLaunch Pix4Dmapper on desktop\nSelect New Project with Camera Rigs\n\nName file FlightDate(YYMMDD)_camera(RE-mx)_FieldName_Pix4D\nCreate In: This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; “Your Field Name” &gt; 1 Folder called FlightDate(YYMMDD)_drone(RE-mx)_FieldName_Pix4D\n\nSelect Images, Click the Add Images… button, file explorer will open.\n\nNavigate to This PC &gt; wldata (Under Network locations) &gt; Field_Data_2022 &gt; UAV_images_by_field_2022 &gt; Select a Field &gt; Select a folder by Date (YYMMDD) and drone type (RE-mx only)\nYou’ll see 10 folders, you will add all the images from all the folders\nCtrl + A to select all of the JPGs in the folder, Click Open (images will be selected, green check will appear) then Click Next &gt;\n\nDefine the Camera Rig\n\nEnsure the Rig Model selected is RedEdge-M, parameters are saved, Click Next &gt;\n\nKeep default Image Properties, Click Next &gt;\nKeep default settings of Select Output Coordinate System, Click Next &gt;\nProcessing Options Template will open, under Standard select Ag Multispectral (Do not check the box next to Start Processing), Click Finish\nAfter the map loads, Import the GCPs\n\nSelect the Project tab\nSelect GCP/MTP Manager…, the GCP Manager Window will open\nIn the GCP Coordinate System click the Edit… button, the Select GCP Coordinate System window will open\nCheck the Advanced Coordinate Options box\n\n\n\nClick the From List… button under the Known Coordinate System [m] search bar, Coordinate System window will open\nSelect the following from the dropdown lists 1. Datum: WGS 1984  2. Coordinate System: WGS 84 (Top of list, look for the globe\n\nIn the GCP/MTP Table click the Import GCPs… button, the Import Ground Control Points window will open\nCoordinates order: Longitude, Latitude, Altitude\nClick Browse…, navigate to This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; GCPs &gt; Select your field’s .txt file, Click Open, Click OK, Click OK. The GCPs will appear as blue crosses on the map\n\n\n\nSave Project!\n\nInitial Processing\n\nClick the Processing tab on the bottom left side of the window, check the box next to 1. Initial Processing (ensure other boxes are unchecked)\nAccept Default options, Start Processing (~30 min)\n\nMarking GCPs\n\nAfter initial processing switch to rayCloud view on the left side of the Pix4D Window, Click on the blue GCP marker and Images will open up on the right (I like to uncheck Cameras and Rays for a cleaner map)\nYou can press hold down to move around the images and zoom in and out to find the GCP (they may not be in every image, that’s ok)\nClick on the center of the GCP to mark it, repeat on 8-15 images. Do this for each GCP.\nIn the Selection panel above the Image panel, Click the Apply button occasionally to update and mark the GCPs\nSelect the Process tab at the top of the window\nSelect Reoptimize (this will take about 10 minutes), a Warning will come up, Click OK\n\n\n\nGenerate a new Quality Report by Clicking the Process tab and Selecting Generate Quality Report (this will take about 15 minutes). Ensure there are green checks next to the 5 parameters in the Quality Check (if not, troubleshoot w/ help from the Pix4D website)\n\n\n\nSave Project!\n\nPoint Cloud and Mesh and DSM, Orthomosaic and Index\n\nCheck the box next to 2. Point Cloud and Mesh and 3. DSM, Orthomsaic and Index in the processing window (be sure to uncheck the previous step so Pix4D doesn’t rerun and take even longer),\nClick Processing Options for step 3. DSM, Orthomosaic and Index\nGo to the Index Calculator tab to calibrate the 10 bands\n\n\n\nSelect the drop down box next to the Correction Type: and select Camera and Sun Irradiance\nClick Calibrate, the calibration image will open a blue box will appear on. Move the box corners to each corner of the square on the calibration panel (if the auto selected image isn’t good quality/shadows/etc. you can Browse at the top of the window and select a better image)\nEnter these reflectance factors into each camera 1. Blue – 0.54011 2. Green – 0.54109 3. Red – 0.53888 4. NIR – 0.53488 5. RedEdge – 0.53795 6. Blue 444 – 0.53983 7. Green 531 – 0.54076 8. Red 650 – 0.53960 9. RedEdge 705 – 0.53825 10. RedEdge 740 – 0.53700\n\n\n\n\nAccept all other default settings and Start processing steps 2. and 3. (~1.5 hrs)\nSave Project, update To Be Stitched List! 😊"
  },
  {
    "objectID": "protocols/Drones/Pix4Dmapper_Stitch_RE-mx/index.html#instructions",
    "href": "protocols/Drones/Pix4Dmapper_Stitch_RE-mx/index.html#instructions",
    "title": "Stitching with Pix4Dmapper",
    "section": "",
    "text": "How to Use Pix4Dmapper to Stitch Drone Images (Micasense Camera – 10 Band (RE-mx))\n\nPix4Dmapper is installed on the lambda2 in the dry lab (NOT the Linux lambda)\nCheck what flights need to be stitched on the To Be Stitched List_2022: Teams &gt; UAV Missions &gt; Files\nLaunch Pix4Dmapper on desktop\nSelect New Project with Camera Rigs\n\nName file FlightDate(YYMMDD)_camera(RE-mx)_FieldName_Pix4D\nCreate In: This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; “Your Field Name” &gt; 1 Folder called FlightDate(YYMMDD)_drone(RE-mx)_FieldName_Pix4D\n\nSelect Images, Click the Add Images… button, file explorer will open.\n\nNavigate to This PC &gt; wldata (Under Network locations) &gt; Field_Data_2022 &gt; UAV_images_by_field_2022 &gt; Select a Field &gt; Select a folder by Date (YYMMDD) and drone type (RE-mx only)\nYou’ll see 10 folders, you will add all the images from all the folders\nCtrl + A to select all of the JPGs in the folder, Click Open (images will be selected, green check will appear) then Click Next &gt;\n\nDefine the Camera Rig\n\nEnsure the Rig Model selected is RedEdge-M, parameters are saved, Click Next &gt;\n\nKeep default Image Properties, Click Next &gt;\nKeep default settings of Select Output Coordinate System, Click Next &gt;\nProcessing Options Template will open, under Standard select Ag Multispectral (Do not check the box next to Start Processing), Click Finish\nAfter the map loads, Import the GCPs\n\nSelect the Project tab\nSelect GCP/MTP Manager…, the GCP Manager Window will open\nIn the GCP Coordinate System click the Edit… button, the Select GCP Coordinate System window will open\nCheck the Advanced Coordinate Options box\n\n\n\nClick the From List… button under the Known Coordinate System [m] search bar, Coordinate System window will open\nSelect the following from the dropdown lists 1. Datum: WGS 1984  2. Coordinate System: WGS 84 (Top of list, look for the globe\n\nIn the GCP/MTP Table click the Import GCPs… button, the Import Ground Control Points window will open\nCoordinates order: Longitude, Latitude, Altitude\nClick Browse…, navigate to This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; GCPs &gt; Select your field’s .txt file, Click Open, Click OK, Click OK. The GCPs will appear as blue crosses on the map\n\n\n\nSave Project!\n\nInitial Processing\n\nClick the Processing tab on the bottom left side of the window, check the box next to 1. Initial Processing (ensure other boxes are unchecked)\nAccept Default options, Start Processing (~30 min)\n\nMarking GCPs\n\nAfter initial processing switch to rayCloud view on the left side of the Pix4D Window, Click on the blue GCP marker and Images will open up on the right (I like to uncheck Cameras and Rays for a cleaner map)\nYou can press hold down to move around the images and zoom in and out to find the GCP (they may not be in every image, that’s ok)\nClick on the center of the GCP to mark it, repeat on 8-15 images. Do this for each GCP.\nIn the Selection panel above the Image panel, Click the Apply button occasionally to update and mark the GCPs\nSelect the Process tab at the top of the window\nSelect Reoptimize (this will take about 10 minutes), a Warning will come up, Click OK\n\n\n\nGenerate a new Quality Report by Clicking the Process tab and Selecting Generate Quality Report (this will take about 15 minutes). Ensure there are green checks next to the 5 parameters in the Quality Check (if not, troubleshoot w/ help from the Pix4D website)\n\n\n\nSave Project!\n\nPoint Cloud and Mesh and DSM, Orthomosaic and Index\n\nCheck the box next to 2. Point Cloud and Mesh and 3. DSM, Orthomsaic and Index in the processing window (be sure to uncheck the previous step so Pix4D doesn’t rerun and take even longer),\nClick Processing Options for step 3. DSM, Orthomosaic and Index\nGo to the Index Calculator tab to calibrate the 10 bands\n\n\n\nSelect the drop down box next to the Correction Type: and select Camera and Sun Irradiance\nClick Calibrate, the calibration image will open a blue box will appear on. Move the box corners to each corner of the square on the calibration panel (if the auto selected image isn’t good quality/shadows/etc. you can Browse at the top of the window and select a better image)\nEnter these reflectance factors into each camera 1. Blue – 0.54011 2. Green – 0.54109 3. Red – 0.53888 4. NIR – 0.53488 5. RedEdge – 0.53795 6. Blue 444 – 0.53983 7. Green 531 – 0.54076 8. Red 650 – 0.53960 9. RedEdge 705 – 0.53825 10. RedEdge 740 – 0.53700\n\n\n\n\nAccept all other default settings and Start processing steps 2. and 3. (~1.5 hrs)\nSave Project, update To Be Stitched List! 😊"
  },
  {
    "objectID": "protocols/DryLab/Rovers/RoverBatteryCharge/index.html",
    "href": "protocols/DryLab/Rovers/RoverBatteryCharge/index.html",
    "title": "Rover Battery Charging SOP",
    "section": "",
    "text": "Rover & Battery\nCharging Batteries\n· The batteries are lithium polymer batteries (LiPo), which can be very dangerous if not properly charged, stored, or mishandled. May result in large and dangerous fires/explosions. If a fire occurs, it is a chemical fire which cannot be easily put out or contained.\nThe biggest warning signs for a potential fire:\no A hissing noise\no A very swollen battery\no Punctures of any sort\no Popping noises\n· When charging/storing/using batteries, make sure there is a “Cold” fire extinguisher (regular fire extinguishers will not work) nearby along with the green metal ammo containers with sand in the bottom and buckets of sand.\no The two buckets have lids that are blue\no If any of the above listed issues occur, 1) quickly place the battery in one of the ammo containers or buckets, 2) dump the (sand) from the other container on top of the battery, and 3) put the lid on top to control the flames, but DO NOT CLOSE THE LID ALL THE WAY (this could turn the container into a bomb).\no PULL FIRE ALARM and/or CALL FIREDEPT IMEDIATLY. If safe to remove ammo can/bucket with the battery in it to outside the build do so.\no Evacuate the building but remain in the building area and look for first responders so you can tell them about the hazard.\no Report the incident to Jacob and Harper so they can file the EHS report.\no File any paperwork needed for incident\n§ EHS link: https://ehs.missouri.edu/work/accident-reporting\n· More to know about LiPo batteries\no https://rogershobbycenter.com/lipoguide\no https://vdr.one/everything-you-need-to-know-about-lipo-batteries/\no https://www.youtube.com/watch?v=ogb0DTqsZEs\no https://www.youtube.com/watch?v=eKLHD7_zzCE&t=0s\no What will happen if the LiPo explodes/catches on fire: https://msadowski.github.io/lipo-safety/#:~:text=If%20your%20battery%20starts%20making,unplug%20it%20from%20the%20charger.\nCharging\n·Each battery has 4 cells that are 20 aH (amp hour).\no https://learn.adafruit.com/all-about-batteries/power-capacity-and-power-capability\n· The max charge for each battery is 16.8 Volts, which is 4.2 Volts per cell.\n· The Venom charger can theoretically charge 4 batteries at once, but we typically only charge two batteries at a time.\n·Each of the four cells in the battery needs to be charged and discharged in a balanced manner (e.g., one cell at 2 Volts and another at 4 Volts is dangerous).\n· Each battery has two cords coming from it. The cord with the think red and black wires and the yellow XT90 end is the main power. The cord with five small wires of different colors is for cell voltage level monitoring. NEVER use or charge the battery without the monitoring cord connected to the charger or on of the small monitor units.\n\nPlace the battery/batteries to be charged in the green metal ammo containers. They should stay in the containers at all times while being charged. Be careful not to get sand in or under the plastic covers on the batteries as this could result in rubbing and damage.\n\n\n\nIf at any time during charging the batteries become hot to the touch, or start to hiss, swell, or make popping noises. As long as it is safe to do so,\n\n\n\nUnplug the battery charger from the wall outlet!\nLeave the battery in the Ammo container and dump sand from the white bucket over the battery.\nFlip the Ammo container lid over the battery but DO NOT close the lid tightly as this could create a bomb.\nPULL FIRE ALARM and/or CALL FIREDEPT IMEDIATLY. If safe to unplug XT90 battery from jumper and remove ammo can/bucket with the battery in it to outside the build do so. If not safe then leave it.\nEvacuate the building but remain in the building area and look for first responders so you can tell them about the hazard.\n\nf.Report the incident to Jacob and Harper so they can file the EHS report.\n\nEnsure that the sperate XT90 jumper line (not the line on the batter itself) is properly plugged into the Venom charger and that the “battery balance” adaptor is also plugged into the charger on the same charging channel as the XT90 jumper. VERY IMPORTANT: make sure the XT90 jumper cables are plugged in correctly (Red to Red, Black to Black). NEVER plug the yellow XT90 end into the battery without having the Red and Black jumper ends plugged into the charger!  If the jumper is plugged into the battery and the Red and Black ends touch each other it could cause a fire! \n\n\n\n4.Turn on the power for the charger; make sure the charger is plugged into an outlet.\n\nUse the “CHANNEL” button to select which of the 4 channels you want to charge the battery on.\nYou want to use the “LiPo BALANCE” program. DO NOT USE the “LiPo CHARGE” program as this will not necessarily charge each cell in a balanced manor. Depending on what was previously selected you may have to use the Stop, Decrease, Increase, and Enter buttons to find the correct program.\n\n\n\n\nWe want to charge with 7.0A, 14.8V, and (4S). If these parameters are not correct you can change them by pressing enter (short press, not long press and hold).\n\n\n\nPlug the battery/Batteries (Both the XT90 and the multicolored monitoring cable into the charger using the jumper lines from 2 above.\n\n\n\nTo charge the batteries, long press the start button. The charger will check that the battery is correctly detected and display both the number of cells. If the number of cells for “R:” and “S:” do not match then something is wrong and you need to recheck the battery connections and settings.\nIf both “R:” and “S:” read “4SER” then you can press short press the ENTER button to begin charging. (if you forget to press enter this second time, the batteries will not charge even if they are plugged in).\nOnce charging begins, DO NOT LEAVE THE ROOM (even for 30 seconds) UNLESS YOU TELL SOMEONE SO THEY CAN WATCH OVER THE BATTERIES\nYou can check the status of the batteries on the displayed screen. Pushing “INC” or “DEC” will take you to a second screen where you can see the status of each of the four cells in the battery.\nAfter a long period of time, the charger may time out and you will need to restart it (press start button twice) if the battery is not all the way charged.\n\n· The cells should always be within 0.1-0.2 of each other; if they are not, let someone know.\n· The charger will get hot so keep it in cool area."
  },
  {
    "objectID": "protocols/DryLab/Rovers/RoverBatteryCharge/index.html#instructions",
    "href": "protocols/DryLab/Rovers/RoverBatteryCharge/index.html#instructions",
    "title": "Rover Battery Charging SOP",
    "section": "",
    "text": "Rover & Battery\nCharging Batteries\n· The batteries are lithium polymer batteries (LiPo), which can be very dangerous if not properly charged, stored, or mishandled. May result in large and dangerous fires/explosions. If a fire occurs, it is a chemical fire which cannot be easily put out or contained.\nThe biggest warning signs for a potential fire:\no A hissing noise\no A very swollen battery\no Punctures of any sort\no Popping noises\n· When charging/storing/using batteries, make sure there is a “Cold” fire extinguisher (regular fire extinguishers will not work) nearby along with the green metal ammo containers with sand in the bottom and buckets of sand.\no The two buckets have lids that are blue\no If any of the above listed issues occur, 1) quickly place the battery in one of the ammo containers or buckets, 2) dump the (sand) from the other container on top of the battery, and 3) put the lid on top to control the flames, but DO NOT CLOSE THE LID ALL THE WAY (this could turn the container into a bomb).\no PULL FIRE ALARM and/or CALL FIREDEPT IMEDIATLY. If safe to remove ammo can/bucket with the battery in it to outside the build do so.\no Evacuate the building but remain in the building area and look for first responders so you can tell them about the hazard.\no Report the incident to Jacob and Harper so they can file the EHS report.\no File any paperwork needed for incident\n§ EHS link: https://ehs.missouri.edu/work/accident-reporting\n· More to know about LiPo batteries\no https://rogershobbycenter.com/lipoguide\no https://vdr.one/everything-you-need-to-know-about-lipo-batteries/\no https://www.youtube.com/watch?v=ogb0DTqsZEs\no https://www.youtube.com/watch?v=eKLHD7_zzCE&t=0s\no What will happen if the LiPo explodes/catches on fire: https://msadowski.github.io/lipo-safety/#:~:text=If%20your%20battery%20starts%20making,unplug%20it%20from%20the%20charger.\nCharging\n·Each battery has 4 cells that are 20 aH (amp hour).\no https://learn.adafruit.com/all-about-batteries/power-capacity-and-power-capability\n· The max charge for each battery is 16.8 Volts, which is 4.2 Volts per cell.\n· The Venom charger can theoretically charge 4 batteries at once, but we typically only charge two batteries at a time.\n·Each of the four cells in the battery needs to be charged and discharged in a balanced manner (e.g., one cell at 2 Volts and another at 4 Volts is dangerous).\n· Each battery has two cords coming from it. The cord with the think red and black wires and the yellow XT90 end is the main power. The cord with five small wires of different colors is for cell voltage level monitoring. NEVER use or charge the battery without the monitoring cord connected to the charger or on of the small monitor units.\n\nPlace the battery/batteries to be charged in the green metal ammo containers. They should stay in the containers at all times while being charged. Be careful not to get sand in or under the plastic covers on the batteries as this could result in rubbing and damage.\n\n\n\nIf at any time during charging the batteries become hot to the touch, or start to hiss, swell, or make popping noises. As long as it is safe to do so,\n\n\n\nUnplug the battery charger from the wall outlet!\nLeave the battery in the Ammo container and dump sand from the white bucket over the battery.\nFlip the Ammo container lid over the battery but DO NOT close the lid tightly as this could create a bomb.\nPULL FIRE ALARM and/or CALL FIREDEPT IMEDIATLY. If safe to unplug XT90 battery from jumper and remove ammo can/bucket with the battery in it to outside the build do so. If not safe then leave it.\nEvacuate the building but remain in the building area and look for first responders so you can tell them about the hazard.\n\nf.Report the incident to Jacob and Harper so they can file the EHS report.\n\nEnsure that the sperate XT90 jumper line (not the line on the batter itself) is properly plugged into the Venom charger and that the “battery balance” adaptor is also plugged into the charger on the same charging channel as the XT90 jumper. VERY IMPORTANT: make sure the XT90 jumper cables are plugged in correctly (Red to Red, Black to Black). NEVER plug the yellow XT90 end into the battery without having the Red and Black jumper ends plugged into the charger!  If the jumper is plugged into the battery and the Red and Black ends touch each other it could cause a fire! \n\n\n\n4.Turn on the power for the charger; make sure the charger is plugged into an outlet.\n\nUse the “CHANNEL” button to select which of the 4 channels you want to charge the battery on.\nYou want to use the “LiPo BALANCE” program. DO NOT USE the “LiPo CHARGE” program as this will not necessarily charge each cell in a balanced manor. Depending on what was previously selected you may have to use the Stop, Decrease, Increase, and Enter buttons to find the correct program.\n\n\n\n\nWe want to charge with 7.0A, 14.8V, and (4S). If these parameters are not correct you can change them by pressing enter (short press, not long press and hold).\n\n\n\nPlug the battery/Batteries (Both the XT90 and the multicolored monitoring cable into the charger using the jumper lines from 2 above.\n\n\n\nTo charge the batteries, long press the start button. The charger will check that the battery is correctly detected and display both the number of cells. If the number of cells for “R:” and “S:” do not match then something is wrong and you need to recheck the battery connections and settings.\nIf both “R:” and “S:” read “4SER” then you can press short press the ENTER button to begin charging. (if you forget to press enter this second time, the batteries will not charge even if they are plugged in).\nOnce charging begins, DO NOT LEAVE THE ROOM (even for 30 seconds) UNLESS YOU TELL SOMEONE SO THEY CAN WATCH OVER THE BATTERIES\nYou can check the status of the batteries on the displayed screen. Pushing “INC” or “DEC” will take you to a second screen where you can see the status of each of the four cells in the battery.\nAfter a long period of time, the charger may time out and you will need to restart it (press start button twice) if the battery is not all the way charged.\n\n· The cells should always be within 0.1-0.2 of each other; if they are not, let someone know.\n· The charger will get hot so keep it in cool area."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_From_Conda/index.html",
    "href": "protocols/Logistics/Container_Singularity_From_Conda/index.html",
    "title": "Converting your Conda Enviroment to a Singularity Container",
    "section": "",
    "text": "For many tasks, building a container is overkill. Using a container will allow the same code to run on an HPC, but for purely local analysis a virtual environment (e.g. using conda, mamba, renv, or packrat) will do just fine. This guide supposes you have an analysis developed in a virtual environment (here I assume using conda) that needs to be containerized to run on a different machine.\nTo start out, we need to export our environment’s packages to a .yml file (gpu.yml). For this example I’m using the gpu environment and export the requirements file below.\nNext, we create a .def file (gpu.def) that contains conda and will download the specified requirements.\nBuild the container like so:\nAnd then we can test that the default python is conda …\n… and that it still can access the host gpus."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_From_Conda/index.html#references",
    "href": "protocols/Logistics/Container_Singularity_From_Conda/index.html#references",
    "title": "Converting your Conda Enviroment to a Singularity Container",
    "section": "References",
    "text": "References\nThe def file used here was modified from this guide (which uses apptainer)."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_Rstudio/index.html",
    "href": "protocols/Logistics/Container_Singularity_Rstudio/index.html",
    "title": "Building an RStudio Singularity Container",
    "section": "",
    "text": "mkdir rstudio_container\ncd rstudio_container\n# \nmkdir -p run var-lib-rstudio-server\nprintf 'provider=sqlite\\ndirectory=/var/lib/rstudio-server\\n' &gt; database.conf\nmkdir home\n\n\n\nRStudio stores your user preferences in rstudio-prefs.json. This file is in AppData/Roaming/RStudio on windows and ~/.config/rstudio on OSX/Linux.\nmkdir ide_settings\nCopy this file to ide_settings. It should look similar to this:\n{\n    \"save_workspace\": \"never\",\n    \"always_save_history\": false,\n    \"reuse_sessions_for_project_links\": true,\n    \"posix_terminal_shell\": \"bash\",\n    \"initial_working_directory\": \"~\",\n    \"panes\": {\n        \"quadrants\": [\n            \"Source\",\n            \"TabSet1\",\n            \"Console\",\n            \"TabSet2\"\n        ],\n        \"tabSet1\": [\n            \"Environment\",\n            \"History\",\n            \"Connections\",\n            \"Build\",\n            \"VCS\",\n            \"Tutorial\",\n            \"Presentation\"\n        ],\n        \"tabSet2\": [\n            \"Files\",\n            \"Plots\",\n            \"Packages\",\n            \"Help\",\n            \"Viewer\",\n            \"Presentations\"\n        ],\n        \"hiddenTabSet\": [],\n        \"console_left_on_top\": false,\n        \"console_right_on_top\": true,\n        \"additional_source_columns\": 0\n    },\n    \"editor_theme\": \"Clouds Midnight\"\n}\n\n\n\nln -s /mnt/c/Users/drk8b9/Documents/LabProtocols/\n(links can be removed with unlink [link name])\n\n\n\nsudo singularity pull docker://rocker/rstudio:4.2 Once this is complete there should be a container file present: rstudio_4.2.sif.\n(for customization construct a .def flie)\nrefer to the Rocker Project for more details on the available containers.\n\n\n\nTo make use of the we need to run the container which will setup the path we need to bind our preferences to.\nsingularity exec \\\n  --bind run:/run \\\n  --bind var-lib-rstudio-server:/var/lib/rstudio-server \\\n  --bind database.conf:/etc/rstudio/database.conf \\\n  --bind home:/home \\\n  rstudio_4.2.sif \\\n  /usr/lib/rstudio-server/bin/rserver \\\n  --www-address=127.0.0.1 \\\n  --www-port=8700 \\\n  --server-user=rstudio\n  \n# note the arguments under --bind can be passed in on one line with ',' separating them. They are included separately to increase readability.\nPress ctrl+c to exit the container. Now this container can be run with --bind ide_settings:/home/rstudio/.config/rstudio/ \\ to use preferred settings."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_Rstudio/index.html#rstudio-recommended-example",
    "href": "protocols/Logistics/Container_Singularity_Rstudio/index.html#rstudio-recommended-example",
    "title": "Building an RStudio Singularity Container",
    "section": "",
    "text": "mkdir rstudio_container\ncd rstudio_container\n# \nmkdir -p run var-lib-rstudio-server\nprintf 'provider=sqlite\\ndirectory=/var/lib/rstudio-server\\n' &gt; database.conf\nmkdir home\n\n\n\nRStudio stores your user preferences in rstudio-prefs.json. This file is in AppData/Roaming/RStudio on windows and ~/.config/rstudio on OSX/Linux.\nmkdir ide_settings\nCopy this file to ide_settings. It should look similar to this:\n{\n    \"save_workspace\": \"never\",\n    \"always_save_history\": false,\n    \"reuse_sessions_for_project_links\": true,\n    \"posix_terminal_shell\": \"bash\",\n    \"initial_working_directory\": \"~\",\n    \"panes\": {\n        \"quadrants\": [\n            \"Source\",\n            \"TabSet1\",\n            \"Console\",\n            \"TabSet2\"\n        ],\n        \"tabSet1\": [\n            \"Environment\",\n            \"History\",\n            \"Connections\",\n            \"Build\",\n            \"VCS\",\n            \"Tutorial\",\n            \"Presentation\"\n        ],\n        \"tabSet2\": [\n            \"Files\",\n            \"Plots\",\n            \"Packages\",\n            \"Help\",\n            \"Viewer\",\n            \"Presentations\"\n        ],\n        \"hiddenTabSet\": [],\n        \"console_left_on_top\": false,\n        \"console_right_on_top\": true,\n        \"additional_source_columns\": 0\n    },\n    \"editor_theme\": \"Clouds Midnight\"\n}\n\n\n\nln -s /mnt/c/Users/drk8b9/Documents/LabProtocols/\n(links can be removed with unlink [link name])\n\n\n\nsudo singularity pull docker://rocker/rstudio:4.2 Once this is complete there should be a container file present: rstudio_4.2.sif.\n(for customization construct a .def flie)\nrefer to the Rocker Project for more details on the available containers.\n\n\n\nTo make use of the we need to run the container which will setup the path we need to bind our preferences to.\nsingularity exec \\\n  --bind run:/run \\\n  --bind var-lib-rstudio-server:/var/lib/rstudio-server \\\n  --bind database.conf:/etc/rstudio/database.conf \\\n  --bind home:/home \\\n  rstudio_4.2.sif \\\n  /usr/lib/rstudio-server/bin/rserver \\\n  --www-address=127.0.0.1 \\\n  --www-port=8700 \\\n  --server-user=rstudio\n  \n# note the arguments under --bind can be passed in on one line with ',' separating them. They are included separately to increase readability.\nPress ctrl+c to exit the container. Now this container can be run with --bind ide_settings:/home/rstudio/.config/rstudio/ \\ to use preferred settings."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_Rstudio/index.html#using-the-container",
    "href": "protocols/Logistics/Container_Singularity_Rstudio/index.html#using-the-container",
    "title": "Building an RStudio Singularity Container",
    "section": "Using the container",
    "text": "Using the container\nsingularity exec \\\n  --bind run:/run \\\n  --bind var-lib-rstudio-server:/var/lib/rstudio-server \\\n  --bind database.conf:/etc/rstudio/database.conf \\\n  --bind home:/home \\\n  --bind LabProtocols:/home/rstudio/LabProtocols \\\n  --bind ide_settings:/home/rstudio/.config/rstudio/ \\\n  rstudio_4.2.sif \\\n  /usr/lib/rstudio-server/bin/rserver \\\n  --www-address=127.0.0.1 \\\n  --www-port=8700 \\\n  --server-user=rstudio\n  \n# note, you can also bind a folder or file by it's full path. For me on WSL this would be\n# --bind /mnt/c/Users/drk8b9/Documents/LabProtocols:/home/rstudio/LabProtocols \\\nLogin with defaults: “rstudio” and “rstudio”."
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html",
    "title": "Generic HPC Workflow",
    "section": "",
    "text": "flowchart LR\n\n%% Parts\nsubgraph PC\n    subgraph PC_Containers\n        def --&gt; sif\n        kern[kernel.json]\n    end\n    subgraph PC_Proj\n        PC_data[/Data/]\n        PC_code[Code]\n    end\n    subgraph PC_Sess\n        PC_shell[Bash session]\n        PC_web[Web browser]\n    end\nend\n\n\nsubgraph HPC\n    HPC_shell[Bash over SSH]\n    ood[Open \\nOnDemand]\n    subgraph HPC_User[\"`home/first.last`\"]\n        HPC_sif\n        HPC_kern[kernel.json]\n    end\n    subgraph HPC_Proj[\"`/project/project_name`\"]\n        HPC_data[/Data/]\n        HPC_code[Code]      \n    end\nend\n\n%% Connections\nPC_data -- dtn --&gt; HPC_data\nPC_code -- dtn --&gt; HPC_code\n\nsif -- dtn --&gt; HPC_sif\nkern -- dtn --&gt; HPC_kern\n\nPC_shell -- login --&gt; HPC_shell\n\nHPC_kern --&gt; ood\nHPC_code --&gt; ood\nHPC_data --&gt; ood \nPC_web --&gt; ood"
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html#tldr",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html#tldr",
    "title": "Generic HPC Workflow",
    "section": "TLDR;",
    "text": "TLDR;\n\nhttps://atlas-ood.hpc.msstate.edu/"
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html#ceres-vs-atlas",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html#ceres-vs-atlas",
    "title": "Generic HPC Workflow",
    "section": "Ceres vs Atlas",
    "text": "Ceres vs Atlas\nMost of our work is done on Atlas so you’ll need to use this url https://atlas-ood.hpc.msstate.edu/. SciNet may direct you to Ceres instead https://ceres-ood.scinet.usda.gov/. If you login to Ceres, you’ll see the same project directories but any data will be missing.\nTo login you’ll need an authenticator code in addition to your login info. Your user name will be the same for both HPCs but the password should differ.\nIn this example I want a gpu compitable container with jupyter allowing deep neural network development on Atlas.\nBare bones .def file\nBootstrap: docker\nFrom: nvcr.io/nvidia/pytorch:23.04-py3"
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html#testing-container",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html#testing-container",
    "title": "Generic HPC Workflow",
    "section": "Testing container:",
    "text": "Testing container:\n\nBuild sandbox container: singularity build --sandbox jnb jupyter.def\nTest for pytorch & jupyter locally: singularity shell jnb python -c “import torch; print( torch.cuda.is_available() )” jupyter-notebook # then check on browser exit\nTest for pytorch on lambda:\nAdd in jupyter:"
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html#finalizing-container",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html#finalizing-container",
    "title": "Generic HPC Workflow",
    "section": "Finalizing container",
    "text": "Finalizing container\n\nFinalize\nAdd"
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html#the-cycle",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html#the-cycle",
    "title": "Generic HPC Workflow",
    "section": "“The cycle”",
    "text": "“The cycle”\n\nIdentify new needs (libraries, tools)\nEdit .def\nVersion control\nbuild container\nSend to HPC\nDevelopment\n\nLocal or on Open OnDemand\n\nRun GPU code\n\nLocal\nExport notebook to txt and run as script."
  },
  {
    "objectID": "protocols/Operations/Authoring_Protocols/index.html",
    "href": "protocols/Operations/Authoring_Protocols/index.html",
    "title": "Authoring Protocols",
    "section": "",
    "text": "Authoring and editing protocols is meant to be easy an accessible. There’s no wrong way to do it so long as you send documentation of the what you want added to a lab member with access to the website. That being said, the form of this documentation falls in different “levels” and the higher the level the faster you’re work will be accessible to everyone 😃. If you send a document it will go through each of these levels before being posted."
  },
  {
    "objectID": "protocols/Operations/Authoring_Protocols/index.html#level-1-word-document",
    "href": "protocols/Operations/Authoring_Protocols/index.html#level-1-word-document",
    "title": "Authoring Protocols",
    "section": "Level 1: Word Document",
    "text": "Level 1: Word Document\n\nHere’s an example protocol. It has clearly written steps, useful images, and an informative title. This is a great start!"
  },
  {
    "objectID": "protocols/Operations/Authoring_Protocols/index.html#level-2-word-document-images-on-the-side",
    "href": "protocols/Operations/Authoring_Protocols/index.html#level-2-word-document-images-on-the-side",
    "title": "Authoring Protocols",
    "section": "Level 2: Word Document, Images on the side",
    "text": "Level 2: Word Document, Images on the side\nBefore a protocol can become a webpage the pictures need to be removed. Unlike a word document which contains the pictures within it, the Quarto documents for this site contain links to the images. For instance, the picture above is ![](Picture1.png) under the hood.\nThe steps to go from Level 1 to Level 2 are:\n\nSave each of the images that were in the document\n\nA handy trick for this is to copy the picture you want from word to powerpoint. Then you can right click and select “Save as Picture” to get the file.\nIf you want annotations (like the red boxes above) you can also do that in powerpoint. Select the image and annotations, right click and select “Group”. Then you can save the whole group as a picture.\n\nIn the place of a picture type ![](PictureName.png) so it’s clear where each picture belongs.\n\nPicture names can be descriptive (“StitchProtocolPage1.png”) or numbered (“Picture1.png”)."
  },
  {
    "objectID": "protocols/Operations/Authoring_Protocols/index.html#level-3-quarto-document-images-on-the-side-in-a-zipped-folder",
    "href": "protocols/Operations/Authoring_Protocols/index.html#level-3-quarto-document-images-on-the-side-in-a-zipped-folder",
    "title": "Authoring Protocols",
    "section": "Level 3: Quarto Document, Images on the side, in a Zipped Folder",
    "text": "Level 3: Quarto Document, Images on the side, in a Zipped Folder\nWhen your protocol is at this level, it’s basically ready for the the website. The big change here is that the protocol is stored in as a “Quarto Markdown File”. Once you’ve completed these steps submit a request to get your 💻🧙 badge.\nBy the end you’ll have a folder with a Quarto file (index.qmd) and several images. The protocol shown above (Stitching Images with Pix4Dmapper), started as a folder called “Pix4Dmapper_Stitch” that looks like this:\n\n\nOpen RStudio and create a new Quarto Document. \nYou’ll see a pop up like this. You can title it now or handle that later.\n\n\n\nRStudio has two visualizing options. Visual and Source. Both are useful but I’d recommend using Visual most of the time.\nVisual:\n\n\nSource:\n\n\nThe ![](Picture1.png) won’t be treated properly if it’s pasted in while RStudio is in “Visual” mode. You can switch into “Source” to correct this or click on the small image icon by “Format” to insert an image.\nThere’s the stitching protocol after I converted it to a Quarto document. One thing to notice is that font color takes some effort. The text that was red in the word document is surrounded by &lt;font color=\"red\"&gt; and &lt;/font&gt;. When this page is converted into html this will be processed and show up nicely (but not until then).\nVisual:\n\n\nSource:\n\n\nSave your file as index.qmd in the same folder as the images for the file.\nFinally, zip this folder and send it to be added to the site!"
  },
  {
    "objectID": "protocols/Rootbot/Restarting_Image_Script_On_Pi/index.html",
    "href": "protocols/Rootbot/Restarting_Image_Script_On_Pi/index.html",
    "title": "Restarting the Rootbot Imaging Script over SSH",
    "section": "",
    "text": "There’s an unresolved memory leak in the imaging script on the Rootbot. Overtime it consumes ever more ram until the script is killed. Ideally we should rewrite the script or monitor it using systemd so that the script is automatically restarted when it crashes. This has not risen to the top of the todo list so the current work around is as follows:\n\nAbout half way through an experiment, ssh into the rootbot’s pi.\nCheck that pictures aren’t currently being taken by looking at the timestamps in the pictures directory (ls -lh ~/Pictures)\nFind the process id number (PID) for the script (\nrootBotPhotoScript_23Feb2021.py\n) (e.g. with top or htop)\nEnd the process with kill $pid where $pid is the value from step 3.\ngo to the folder with the script (cd \\~/rootbot/jupyter_notebooks).\nRestart the rootbot script (rootBotPhotoScript_23Feb2021.py). Don’t be fooled by the shebang line, python3 must be explicitly used. The command to do this is nohup python3 rootBotPhotoScript_23Feb2021.py &&. nohup and && ensure the process continues in the background after the terminal is closed. python3 ensures the system’s default python (2) isn’t used. The full path need not be provided for the script because the present working directory is the /jupyter_notebooks folder."
  }
]